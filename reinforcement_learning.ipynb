{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "This Jupyter notebook acts as supporting material for **Chapter 21 Reinforcement Learning** of the book* Artificial Intelligence: A Modern Approach*. This notebook makes use of the implementations in `rl.py` module. We also make use of implementation of MDPs in the `mdp.py` module to test our agents. It might be helpful if you have already gone through the Jupyter notebook dealing with Markov decision process. Let us import everything from the `rl` module. It might be helpful to view the source of some of our implementations. Please refer to the Introductory Jupyter notebook for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reinforcement_learning4e import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONTENTS\n",
    "\n",
    "* Overview\n",
    "* Passive Reinforcement Learning\n",
    "    - Direct Utility Estimation\n",
    "    - Adaptive Dynamic Programming\n",
    "    - Temporal-Difference Agent\n",
    "* Active Reinforcement Learning\n",
    "    - Q learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## OVERVIEW\n",
    "\n",
    "Before we start playing with the actual implementations let us review a couple of things about RL.\n",
    "\n",
    "1. Reinforcement Learning is concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. \n",
    "\n",
    "2. Reinforcement learning differs from standard supervised learning in that correct input/output pairs are never presented, nor sub-optimal actions explicitly corrected. Further, there is a focus on on-line performance, which involves finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).\n",
    "\n",
    "-- Source: [Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning)\n",
    "\n",
    "In summary we have a sequence of state action transitions with rewards associated with some states. Our goal is to find the optimal policy $\\pi$ which tells us what action to take in each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PASSIVE REINFORCEMENT LEARNING\n",
    "\n",
    "In passive Reinforcement Learning the agent follows a fixed policy $\\pi$. Passive learning attempts to evaluate the given policy $pi$ - without any knowledge of the Reward function $R(s)$ and the Transition model $P(s'\\ |\\ s, a)$.\n",
    "\n",
    "This is usually done by some method of **utility estimation**. The agent attempts to directly learn the utility of each state that would result from following the policy. Note that at each step, it has to *perceive* the reward and the state - it has no global knowledge of these. Thus, if a certain the entire set of actions offers a very low probability of attaining some state $s_+$ - the agent may never perceive the reward $R(s_+)$.\n",
    "\n",
    "Consider a situation where an agent is given a policy to follow. Thus, at any point it knows only its current state and current reward, and the action it must take next. This action may lead it to more than one state, with different probabilities.\n",
    "\n",
    "For a series of actions given by $\\pi$, the estimated utility $U$:\n",
    "$$U^{\\pi}(s) = E(\\sum_{t=0}^\\inf \\gamma^t R^t(s')$$)\n",
    "Or the expected value of summed discounted rewards until termination.\n",
    "\n",
    "Based on this concept, we discuss three methods of estimating utility:\n",
    "\n",
    "1. **Direct Utility Estimation (DUE)**\n",
    " \n",
    " The first, most naive method of estimating utility comes from the simplest interpretation of the above definition. We construct an agent that follows the policy until it reaches the terminal state. At each step, it logs its current state, reward. Once it reaches the terminal state, it can estimate the utility for each state for *that* iteration, by simply summing the discounted rewards from that state to the terminal one.\n",
    "\n",
    " It can now run this 'simulation' $n$ times, and calculate the average utility of each state. If a state occurs more than once in a simulation, both its utility values are counted separately.\n",
    " \n",
    " Note that this method may be prohibitively slow for very large statespaces. Besides, **it pays no attention to the transition probability $P(s'\\ |\\ s, a)$.** It misses out on information that it is capable of collecting (say, by recording the number of times an action from one state led to another state). The next method addresses this issue.\n",
    " \n",
    "2. **Adaptive Dynamic Programming (ADP)**\n",
    " \n",
    " This method makes use of knowledge of the past state $s$, the action $a$, and the new perceived state $s'$ to estimate the transition probability $P(s'\\ |\\ s,a)$. It does this by the simple counting of new states resulting from previous states and actions.<br> \n",
    " The program runs through the policy a number of times, keeping track of:\n",
    "    - each occurrence of state $s$ and the policy-recommended action $a$ in $N_{sa}$\n",
    "    - each occurrence of $s'$ resulting from $a$ on $s$ in $N_{s'|sa}$.\n",
    "     \n",
    " It can thus estimate $P(s'\\ |\\ s,a)$ as $N_{s'|sa}/N_{sa}$, which in the limit of infinite trials, will converge to the true value.<br>\n",
    " Using the transition probabilities thus estimated, it can apply `POLICY-EVALUATION` to estimate the utilities $U(s)$ using properties of convergence of the Bellman functions.\n",
    "\n",
    "3. **Temporal-difference learning (TD)**\n",
    " \n",
    " Instead of explicitly building the transition model $P$, the temporal-difference model makes use of the expected closeness between the utilities of two consecutive states $s$ and $s'$.\n",
    " For the transition $s$ to $s'$, the update is written as:\n",
    "$$U^{\\pi}(s) \\leftarrow U^{\\pi}(s) + \\alpha \\left( R(s) + \\gamma U^{\\pi}(s') - U^{\\pi}(s) \\right)$$\n",
    " This model implicitly incorporates the transition probabilities by being weighed for each state by the number of times it is achieved from the current state. Thus, over a number of iterations, it converges similarly to the Bellman equations.\n",
    " The advantage of the TD learning model is its relatively simple computation at each step, rather than having to keep track of various counts.\n",
    " For $n_s$ states and $n_a$ actions the ADP model would have $n_s \\times n_a$ numbers $N_{sa}$ and $n_s^2 \\times n_a$ numbers $N_{s'|sa}$ to keep track of. The TD model must only keep track of a utility $U(s)$ for each state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Demonstrating Passive agents\n",
    "\n",
    "Passive agents are implemented in `rl.py` as various `Agent-Class`es.\n",
    "\n",
    "To demonstrate these agents, we make use of the `GridMDP` object from the `MDP` module. `sequential_decision_environment` is similar to that used for the `MDP` notebook but has discounting with $\\gamma = 0.9$.\n",
    "\n",
    "The `Agent-Program` can be obtained by creating an instance of the relevant `Agent-Class`. The `__call__` method allows the `Agent-Class` to be called as a function. The class needs to be instantiated with a policy ($\\pi$) and an `MDP` whose utility of states will be estimated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp4e import sequential_decision_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sequential_decision_environment` is a GridMDP object as shown below. The rewards are **+1** and **-1** in the terminal states, and **-0.04** in the rest. <img src=\"files/images/mdp.png\"> Now we define actions and a policy similar to **Fig 21.1** in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Directions\n",
    "north = (0, 1)\n",
    "south = (0,-1)\n",
    "west = (-1, 0)\n",
    "east = (1, 0)\n",
    "\n",
    "# policy = {\n",
    "#     (0, 2): east, (1, 2): east,  (2, 2): east,   (3, 2): None,\n",
    "#     (0, 1): north,               (2, 1): north,    (3, 1): None,\n",
    "#     (0, 0): north, (1, 0): west,                   (3, 0): north, \n",
    "# }\n",
    "\n",
    "policy = {\n",
    "    (0, 7): east,  (1, 7): east,  (2, 7): east,   (3, 7): east, (4, 7): east,  (5, 7): east,  (6, 7): east,   (7, 7): None,\n",
    "    (0, 6): north, (1, 6): north, (2, 6): north,  (3, 6): north, (4, 6): east,  (5, 6): east,  (6, 6): east,   (7, 6): north,\n",
    "    (0, 5): north, (1, 5): north,  (2, 5): west,   (3, 5): west, (4, 5): east,  (5, 5): north, (6, 5): east,    (7, 5): north,\n",
    "    (0, 4): north, (1, 4): north, (2, 4): north,  (3, 4): north,                                               (7, 4): north,\n",
    "    (0, 3): north, (1, 3): north, (2, 3): north,  (3, 3): north, (4, 3): east,  (5, 3): east,  (6, 3): east,   (7, 3): north,\n",
    "    (0, 2): north, (1, 2): north, (2, 2): north,   (3, 2): west, (4, 2): east,  (5, 2): east,  (6, 2): east,   (7, 2): north,\n",
    "    (0, 1): east, (1, 1): east, (2, 1): north,   (3, 1): east,  (4, 1): east,  (5, 1): east,  (6, 1): east,   (7, 1): north,\n",
    "    (0, 0): east, (1, 0): east,  (2, 0): east,   (3, 0): east, (4, 0): east,  (5, 0): east,  (6, 0): east,   (7, 0): north\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Direction Utility Estimation Agent\n",
    "\n",
    "The `PassiveDEUAgent` class in the `rl` module implements the Agent Program described in **Fig 21.2** of the AIMA Book. `PassiveDEUAgent` sums over rewards to find the estimated utility for each state. It thus requires the running of a number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mclass\u001b[0m \u001b[0mPassiveDUEAgent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m    Passive (non-learning) agent that uses direct utility estimation\u001b[0m\n",
      "\u001b[0;34m    on a given MDP and policy.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    import sys\u001b[0m\n",
      "\u001b[0;34m    from mdp import sequential_decision_environment\u001b[0m\n",
      "\u001b[0;34m    north = (0, 1)\u001b[0m\n",
      "\u001b[0;34m    south = (0,-1)\u001b[0m\n",
      "\u001b[0;34m    west = (-1, 0)\u001b[0m\n",
      "\u001b[0;34m    east = (1, 0)\u001b[0m\n",
      "\u001b[0;34m    policy = {(0, 2): east, (1, 2): east, (2, 2): east, (3, 2): None, (0, 1): north, (2, 1): north,\u001b[0m\n",
      "\u001b[0;34m              (3, 1): None, (0, 0): north, (1, 0): west, (2, 0): west, (3, 0): west,}\u001b[0m\n",
      "\u001b[0;34m    agent = PassiveDUEAgent(policy, sequential_decision_environment)\u001b[0m\n",
      "\u001b[0;34m    for i in range(200):\u001b[0m\n",
      "\u001b[0;34m        run_single_trial(agent,sequential_decision_environment)\u001b[0m\n",
      "\u001b[0;34m        agent.estimate_U()\u001b[0m\n",
      "\u001b[0;34m    agent.U[(0, 0)] > 0.2\u001b[0m\n",
      "\u001b[0;34m    True\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpercept\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0ms1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mestimate_U\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# this function can be called only if the MDP has reached a terminal state\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# it will also reset the mdp history\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MDP is not in terminal state'\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_history\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# calculating the utilities based on the current iteration\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mU2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mU2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_history\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mU2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mU2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# resetting history\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# setting the new utilities to the average of the previous\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# iteration and this one\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mU2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mU2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"To be overridden in most cases. The default case\u001b[0m\n",
      "\u001b[0;34m        assumes the percept to be of type (state, reward)\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mpercept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%psource PassiveDUEAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUEagent = PassiveDUEAgent(policy, sequential_decision_environment)\n",
    "for i in range(1000):\n",
    "    run_single_trial(DUEagent, sequential_decision_environment)\n",
    "    DUEagent.estimate_U()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculated utilities are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 4):-3.68445816478141\n",
      "(4, 0):-8.242804181826422\n",
      "(7, 1):-7.439082388829168\n",
      "(7, 7):10.0\n",
      "(6, 5):9.044490591880303\n",
      "(0, 0):-8.379597076754223\n",
      "(6, 1):-3.4244806508907817\n",
      "(7, 0):-7.637610142926445\n",
      "(2, 0):-8.161665537252677\n",
      "(3, 0):-8.121580802042345\n",
      "(7, 3):-7.330553898429667\n",
      "(7, 6):9.959597943623109\n",
      "(5, 0):-9.016138843197512\n",
      "(7, 2):-7.449023214683534\n",
      "(6, 0):-8.948805552897648\n",
      "(1, 0):-8.206688512566645\n",
      "(7, 5):8.926233085980463\n",
      "(3, 7):9.822609171422284\n",
      "(5, 7):9.91352527050985\n",
      "(0, 5):9.504632625617088\n",
      "(2, 2):-2.7651368162022796\n",
      "(1, 6):8.686642655868884\n",
      "(2, 5):-2.3407995129560617\n",
      "(2, 4):-3.2790394387086756\n",
      "(2, 1):-2.8052317482901508\n",
      "(2, 7):9.521972794081718\n",
      "(1, 5):7.685132725118413\n",
      "(6, 7):9.959999799726067\n",
      "(4, 7):9.863392890252129\n",
      "(0, 6):9.544978559839155\n",
      "(2, 3):-2.7592987596306906\n",
      "(1, 7):9.71252241520818\n",
      "(6, 6):9.916132807633652\n",
      "(1, 1):-0.15431776089956128\n",
      "(6, 2):-5.854158418367646\n",
      "(6, 3):-3.8484021573105625\n",
      "(2, 6):8.509033487458847\n",
      "(4, 6):9.754774688482284\n",
      "(5, 6):9.85968623874258\n",
      "(5, 1):-3.8077597527797042\n",
      "(4, 1):-3.8550057622382434\n",
      "(0, 1):-5.755018624195516\n",
      "(3, 4):-1.0412510681152343\n",
      "(3, 1):-4.410367419747187\n",
      "(3, 3):-1.098388671875\n",
      "(3, 2):2.259549236297608\n",
      "(3, 5):-1.0\n",
      "(1, 4):8.272724600303281\n",
      "(1, 3):8.276882176138333\n",
      "(1, 2):5.905260581744715\n",
      "(0, 7):9.636208950440341\n",
      "(0, 2):-10.0\n",
      "(0, 4):9.095976562499999\n",
      "(0, 3):8.0515625\n",
      "(5, 2):-10.0\n",
      "(3, 6):9.719972284808755\n",
      "(4, 3):-3.6499999999999995\n",
      "(5, 3):-3.609999999999999\n",
      "(4, 2):-7.59415771484375\n",
      "(4, 5):9.7275\n",
      "(5, 5):9.80875\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join([str(k)+':'+str(v) for k, v in DUEagent.U.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Dynamic Programming Agent\n",
    "\n",
    "The `PassiveADPAgent` class in the `rl` module implements the Agent Program described in **Fig 21.2** of the AIMA Book. `PassiveADPAgent` uses state transition and occurrence counts to estimate $P$, and then $U$. Go through the source below to understand the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mclass\u001b[0m \u001b[0mPassiveADPAgent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m    [Figure 21.2]\u001b[0m\n",
      "\u001b[0;34m    Passive (non-learning) agent that uses adaptive dynamic programming\u001b[0m\n",
      "\u001b[0;34m    on a given MDP and policy.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    import sys\u001b[0m\n",
      "\u001b[0;34m    from mdp import sequential_decision_environment\u001b[0m\n",
      "\u001b[0;34m    north = (0, 1)\u001b[0m\n",
      "\u001b[0;34m    south = (0,-1)\u001b[0m\n",
      "\u001b[0;34m    west = (-1, 0)\u001b[0m\n",
      "\u001b[0;34m    east = (1, 0)\u001b[0m\n",
      "\u001b[0;34m    policy = {(0, 2): east, (1, 2): east, (2, 2): east, (3, 2): None, (0, 1): north, (2, 1): north,\u001b[0m\n",
      "\u001b[0;34m              (3, 1): None, (0, 0): north, (1, 0): west, (2, 0): west, (3, 0): west,}\u001b[0m\n",
      "\u001b[0;34m    agent = PassiveADPAgent(policy, sequential_decision_environment)\u001b[0m\n",
      "\u001b[0;34m    for i in range(100):\u001b[0m\n",
      "\u001b[0;34m        run_single_trial(agent,sequential_decision_environment)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    agent.U[(0, 0)] > 0.2\u001b[0m\n",
      "\u001b[0;34m    True\u001b[0m\n",
      "\u001b[0;34m    agent.U[(0, 1)] > 0.2\u001b[0m\n",
      "\u001b[0;34m    True\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mclass\u001b[0m \u001b[0mModelMDP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMDP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Class for implementing modified Version of input MDP with\u001b[0m\n",
      "\u001b[0;34m        an editable transition model P and a custom function T.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mnested_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnested_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# StackOverflow:whats-the-best-way-to-initialize-a-dict-of-dicts-in-python\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mdef\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m\"\"\"Return a list of tuples with probabilities for states\u001b[0m\n",
      "\u001b[0;34m            based on the learnt model P.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPassiveADPAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelMDP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactlist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                            \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNs1_sa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisited\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# keeping track of visited states\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpercept\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mmdp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmdp\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNsa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNs1_sa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNsa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNs1_sa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0ms1\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisited\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Reward is only known for visited state.\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisited\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mNsa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mNs1_sa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# for each t such that Nsâ€²|sa [t, s, a] is nonzero\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mNs1_sa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                      \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNs1_sa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mNsa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_evaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m##\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNsa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNs1_sa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNsa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNs1_sa\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0ms1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mterminals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"To be overridden in most cases. The default case\u001b[0m\n",
      "\u001b[0;34m        assumes the percept to be of type (state, reward).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mpercept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%psource PassiveADPAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We instantiate a `PassiveADPAgent` below with the `GridMDP` shown and train it over 200 iterations. The `rl` module has a simple implementation to simulate iterations. The function is called **run_single_trial**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Transition table is empty.\n"
     ]
    }
   ],
   "source": [
    "ADPagent = PassiveADPAgent(policy, sequential_decision_environment)\n",
    "for i in range(200):\n",
    "    run_single_trial(ADPagent, sequential_decision_environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculated utilities are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0):-2.7298817953474193\n",
      "(4, 0):-3.4517194470187684\n",
      "(3, 4):-0.9400000000000001\n",
      "(4, 3):-5.592714725066126\n",
      "(3, 1):-4.319822932305736\n",
      "(3, 7):5.438441868127796\n",
      "(4, 6):5.642090853269927\n",
      "(5, 1):-5.596927891194049\n",
      "(5, 7):7.3168021923224975\n",
      "(0, 2):-10.0\n",
      "(0, 5):2.4145106595552646\n",
      "(2, 2):-4.924468554066622\n",
      "(1, 0):-2.8239739509703212\n",
      "(1, 6):3.1570692380183596\n",
      "(2, 5):-9.692486201754793\n",
      "(1, 3):-0.08289732994938578\n",
      "(7, 4):-5.9855887635099965\n",
      "(6, 2):-5.970086801004628\n",
      "(7, 1):-4.9968809522058315\n",
      "(7, 7):10.0\n",
      "(6, 5):5.5585720406806205\n",
      "(4, 2):-9.008360813139879\n",
      "(3, 0):-3.225432010667912\n",
      "(4, 5):5.037881767942935\n",
      "(3, 3):-1.583907208759919\n",
      "(5, 0):-3.688084723177215\n",
      "(5, 6):6.5850173177601246\n",
      "(3, 6):4.900418702971996\n",
      "(5, 3):-6.169683027842709\n",
      "(0, 1):-4.560561511839428\n",
      "(0, 7):3.087251555275052\n",
      "(2, 4):-8.487355668633054\n",
      "(1, 2):-3.6247197121103194\n",
      "(0, 4):2.133059593599738\n",
      "(2, 1):-4.285944114087199\n",
      "(2, 7):4.60076297164126\n",
      "(1, 5):1.2140339749133124\n",
      "(6, 1):-4.56089870782727\n",
      "(7, 0):-4.370037688478144\n",
      "(7, 3):-6.397259084096448\n",
      "(6, 7):8.474747632402922\n",
      "(7, 6):8.531338629639212\n",
      "(3, 2):-4.47202169865996\n",
      "(4, 1):-5.348841015606563\n",
      "(4, 7):6.231071613146359\n",
      "(3, 5):-1.0\n",
      "(5, 2):-10.0\n",
      "(5, 5):0.0\n",
      "(1, 1):-3.7671676015895907\n",
      "(0, 3):0.9197536342397643\n",
      "(2, 0):-3.065026461945252\n",
      "(1, 4):-0.13561072983270986\n",
      "(0, 6):2.7272340666821004\n",
      "(2, 3):-5.758902884386301\n",
      "(1, 7):3.788465977694956\n",
      "(2, 6):3.064255829755922\n",
      "(7, 2):-5.689522139062064\n",
      "(6, 0):-3.9453396100928138\n",
      "(6, 6):7.372212393253301\n",
      "(7, 5):6.18893931028105\n",
      "(6, 3):-5.7440922531555225\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join([str(k)+':'+str(v) for k, v in ADPagent.U.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Passive Temporal Difference Agent\n",
    "\n",
    "`PassiveTDAgent` uses temporal differences to learn utility estimates. We learn the difference between the states and backup the values to previous states.  Let us look into the source before we see some usage examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mclass\u001b[0m \u001b[0mPassiveTDAgent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m    [Figure 21.4]\u001b[0m\n",
      "\u001b[0;34m    The abstract class for a Passive (non-learning) agent that uses\u001b[0m\n",
      "\u001b[0;34m    temporal differences to learn utility estimates. Override update_state\u001b[0m\n",
      "\u001b[0;34m    method to convert percept to state and reward. The mdp being provided\u001b[0m\n",
      "\u001b[0;34m    should be an instance of a subclass of the MDP Class.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    import sys\u001b[0m\n",
      "\u001b[0;34m    from mdp import sequential_decision_environment\u001b[0m\n",
      "\u001b[0;34m    north = (0, 1)\u001b[0m\n",
      "\u001b[0;34m    south = (0,-1)\u001b[0m\n",
      "\u001b[0;34m    west = (-1, 0)\u001b[0m\n",
      "\u001b[0;34m    east = (1, 0)\u001b[0m\n",
      "\u001b[0;34m    policy = {(0, 2): east, (1, 2): east, (2, 2): east, (3, 2): None, (0, 1): north, (2, 1): north,\u001b[0m\n",
      "\u001b[0;34m              (3, 1): None, (0, 0): north, (1, 0): west, (2, 0): west, (3, 0): west,}\u001b[0m\n",
      "\u001b[0;34m    agent = PassiveTDAgent(policy, sequential_decision_environment, alpha=lambda n: 60./(59+n))\u001b[0m\n",
      "\u001b[0;34m    for i in range(200):\u001b[0m\n",
      "\u001b[0;34m        run_single_trial(agent,sequential_decision_environment)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    agent.U[(0, 0)] > 0.2\u001b[0m\n",
      "\u001b[0;34m    True\u001b[0m\n",
      "\u001b[0;34m    agent.U[(0, 1)] > 0.2\u001b[0m\n",
      "\u001b[0;34m    True\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminals\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# udacity video\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminals\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mNs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0ms1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mterminals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"To be overridden in most cases. The default case\u001b[0m\n",
      "\u001b[0;34m        assumes the percept to be of type (state, reward).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mpercept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%psource PassiveTDAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In creating the `TDAgent`, we use the **same learning rate** $\\alpha$ as given in the footnote of the book on **page 837**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDagent = PassiveTDAgent(policy, sequential_decision_environment, alpha = lambda n: 60./(59+n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run **200 trials** for the agent to estimate Utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    run_single_trial(TDagent,sequential_decision_environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculated utilities are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 0):-3.8454502930340495\n",
      "(3, 4):-0.9393965998882593\n",
      "(4, 3):-4.911164248158563\n",
      "(3, 1):-4.920910539148444\n",
      "(3, 7):4.6673866326548525\n",
      "(4, 6):5.76962866908318\n",
      "(5, 1):-4.835793796699703\n",
      "(5, 7):6.916248428472059\n",
      "(0, 2):-10\n",
      "(0, 5):2.6379092053701143\n",
      "(2, 2):-4.199387237136291\n",
      "(1, 0):-3.377013220342686\n",
      "(1, 6):3.930938195490529\n",
      "(2, 5):-8.32146246607162\n",
      "(1, 3):0.7513281754248284\n",
      "(7, 4):-7.106405678715151\n",
      "(6, 2):-5.566068442072144\n",
      "(7, 1):-5.354256748403494\n",
      "(7, 7):10\n",
      "(6, 5):5.34060587137639\n",
      "(4, 2):-10.0\n",
      "(3, 0):-3.492665432094761\n",
      "(4, 5):-0.07600000000000001\n",
      "(3, 3):-0.848536312031463\n",
      "(5, 0):-3.83158901947671\n",
      "(5, 6):5.8586328071009985\n",
      "(3, 6):4.8762246668103435\n",
      "(5, 3):-6.843624431363199\n",
      "(0, 1):-8.271002746127358\n",
      "(0, 7):4.060441169426265\n",
      "(2, 4):-8.560900321727217\n",
      "(1, 2):-5.439839446577694\n",
      "(0, 4):2.0862857246217548\n",
      "(2, 1):-2.1127657625066427\n",
      "(2, 7):4.555070272158578\n",
      "(1, 5):2.228414129368392\n",
      "(6, 1):-4.8543117798260855\n",
      "(7, 0):-4.719827768270907\n",
      "(7, 3):-6.544447989838503\n",
      "(6, 7):8.959096527439822\n",
      "(7, 6):8.39719457693133\n",
      "(3, 2):-4.926161456252665\n",
      "(4, 1):-4.971360245048218\n",
      "(4, 7):4.778227117551489\n",
      "(3, 5):-1\n",
      "(5, 2):-10\n",
      "(5, 5):1.168783745298712\n",
      "(0, 0):-3.3987037871939827\n",
      "(1, 1):-4.115705498503826\n",
      "(0, 3):1.481801243345477\n",
      "(2, 0):-3.389864979993845\n",
      "(1, 4):1.394150854862981\n",
      "(0, 6):3.199186835869203\n",
      "(2, 3):-7.613560777861116\n",
      "(1, 7):4.347027004416065\n",
      "(2, 6):2.808972415681997\n",
      "(7, 2):-5.666998931132548\n",
      "(6, 0):-4.1904315119459605\n",
      "(6, 6):6.9473554556547406\n",
      "(7, 5):6.021057875965696\n",
      "(6, 3):-5.948959038918744\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join([str(k)+':'+str(v) for k, v in TDagent.U.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with value iteration method\n",
    "\n",
    "We can also compare the utility estimates learned by our agent to those obtained via **value iteration**.\n",
    "\n",
    "**Note that value iteration has a priori knowledge of the transition table $P$, the rewards $R$, and all the states $s$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mdp4e import value_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values calculated by value iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 0):0.20613401404876092\n",
      "(3, 4):0.5027823798502125\n",
      "(4, 3):0.4610575123374177\n",
      "(3, 1):0.3890609707415478\n",
      "(3, 7):5.4768043075879635\n",
      "(4, 6):5.7140678387889325\n",
      "(5, 1):-0.9808577365113227\n",
      "(5, 7):7.419331860809735\n",
      "(0, 2):-10\n",
      "(0, 5):2.3882093486949305\n",
      "(2, 2):0.6277953174566747\n",
      "(1, 0):0.26147733558027636\n",
      "(1, 6):3.3139422080663548\n",
      "(2, 5):-7.807981569055003\n",
      "(1, 3):1.266195840691091\n",
      "(7, 4):-6.615198954253358\n",
      "(6, 2):-1.3214139727971803\n",
      "(7, 1):-0.13331442187882087\n",
      "(7, 7):10\n",
      "(6, 5):6.466073568420779\n",
      "(4, 2):-0.5720032829935289\n",
      "(3, 0):0.2901870316096985\n",
      "(4, 5):5.047704330354068\n",
      "(3, 3):0.7097815079375986\n",
      "(5, 0):0.022130415536142527\n",
      "(5, 6):6.561871328704624\n",
      "(3, 6):4.689724230968557\n",
      "(5, 3):-1.7231194166079322\n",
      "(0, 1):-1.0215163320369014\n",
      "(0, 7):3.3426188835734703\n",
      "(2, 4):-0.15011867142821966\n",
      "(1, 2):-0.4081752711255061\n",
      "(0, 4):2.006145384148317\n",
      "(2, 1):0.4728825757574549\n",
      "(2, 7):4.588415522502781\n",
      "(1, 5):1.1638470983220925\n",
      "(6, 1):-0.17153477862006347\n",
      "(7, 0):-0.09149206028607111\n",
      "(7, 3):-1.4351416381525968\n",
      "(6, 7):8.612532741324225\n",
      "(7, 6):8.612532741324225\n",
      "(3, 2):0.5109083996056243\n",
      "(4, 1):0.20719548355751605\n",
      "(4, 7):6.3914121376637505\n",
      "(3, 5):-1\n",
      "(5, 2):-10\n",
      "(5, 5):5.721034493237777\n",
      "(0, 0):0.06189755781170282\n",
      "(1, 1):0.2872725159659054\n",
      "(0, 3):0.6136069248901318\n",
      "(2, 0):0.3501251023649489\n",
      "(1, 4):1.6231285405746498\n",
      "(0, 6):2.9285059284573984\n",
      "(2, 3):0.9146518939573437\n",
      "(1, 7):3.9141911812390022\n",
      "(2, 6):3.023989155714729\n",
      "(7, 2):-0.2801342719110172\n",
      "(6, 0):-0.04341262432552723\n",
      "(6, 6):7.526719940056038\n",
      "(7, 5):6.354912302100343\n",
      "(6, 3):-1.1584008053380839\n"
     ]
    }
   ],
   "source": [
    "U_values = value_iteration(sequential_decision_environment)\n",
    "print('\\n'.join([str(k)+':'+str(v) for k, v in U_values.items()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolution of utility estimates over iterations\n",
    "\n",
    "We can explore how these estimates vary with time by using plots similar to **Fig 21.5a**. We will first enable matplotlib using the inline backend. We also define a function to collect the values of utilities at each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def graph_utility_estimates(agent_program, mdp, no_of_iterations, states_to_graph):\n",
    "    graphs = {state:[] for state in states_to_graph}\n",
    "    for iteration in range(1,no_of_iterations+1):\n",
    "        run_single_trial(agent_program, mdp)\n",
    "        for state in states_to_graph:\n",
    "            graphs[state].append((iteration, agent_program.U[state]))\n",
    "    for state, value in graphs.items():\n",
    "        state_x, state_y = zip(*value)\n",
    "        plt.plot(state_x, state_y, label=str(state))\n",
    "#     plt.ylim([0,1.2])\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('U')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a plot of state $(2,2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent = PassiveTDAgent(policy, sequential_decision_environment, alpha=lambda n: 60./(59+n))\n",
    "graph_utility_estimates(agent, sequential_decision_environment, 500, [(2,2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to plot multiple states on the same plot. As expected, the utility of the finite state $(3,2)$ stays constant and is equal to $R((3,2)) = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAABEt0lEQVR4nO2deZgcZZ34P2/fc8/kvhMCCeEKIYT7llNBOcRjYT0WV9QVj0UFb1ddVrx3/emK7IKogKC4CKIIBhAQ5AgQkhAgEMgxOSeZ++ruqnp/f1RVT89M90x3T1d3T9f38zzzzHR1ddVb01Xv9/3eSmuNIAiC4D8C5R6AIAiCUB5EAAiCIPgUEQCCIAg+RQSAIAiCTxEBIAiC4FNC5R5APkybNk0vWrSo3MMQBEGYVDz33HP7tNbTR26fVAJg0aJFrFmzptzDEARBmFQopbZm2i4mIEEQBJ8iAkAQBMGniAAQBEHwKSIABEEQfIoIAEEQBJ8iAkAQBMGniAAQBEHwKZMqD0AQykbfPlhzM5hJUAFYcRm0LCz3qARhQogAEIRc2Ph7eOS6odeWAWd+pWzDEYRiICYgQcgFI2H/vnYrhGJgJcs7HkEoAiIABCEXLMP+HQjZJiBtlXc8glAERAAIQi64K/6UAJBWqsLkRwSAIOSCZdq/RQMQqggRAIKQCykTUNAWAK5AEIRJjAgAQcgFM+ms/pVoAELVIAJAEHLBMmwBACIAhKpBBIAg5IJlQiBs/y0CQKgSRAAIQi5Yhm3/B/u3Fh+AMPkRASAIuWAlxQQkVB0iAAQhF0b5ACQPQJj8iAAQhFywTAim+QAkDFSoAkQACEIumMkhH4CYgIQqQQSAIOSChIEKVYgIAEHIBcuQMFCh6hABIAi5YJlDGoCEgQpVgggAQcgFS3wAQvUhAkAQcsEyhkcBSRioUAWIABCEXBjpBJYwUKEKEAEgCLlgShSQUH2IABCEXEivBSQCQKgSRAAIQi5IGKhQhYgAEIRcSC8GJ2GgQpUgAkAQcsEyxQQkVB0iAAQhFyQMVKhCyiIAlFLfVUq9opRap5S6WynVXI5xCELOSBioUIWUSwP4C3C41no5sAn4QpnGIQi5IWGgQhUSKsdJtdYPpr18Cri0HOMQhJyRMNDJR+sa2PG8/XfDLDj0HeUdTwVSFgEwgiuAO7O9qZS6ErgSYMGCBaUakyAMR8JAJx/3XAVtLw+9/txmqJtWvvFUIJ6ZgJRSq5VSGzL8XJi2z5cAA7gt23G01jdqrVdprVdNnz7dq+EKwthIGOjkw4zDsgvg3P+wXxvx8o6nAvFMA9BanzXW+0qpDwIXAGdqLSEVQoWTXg5aNIDJgdYQroVoo/NahPZIymICUkqdB1wDnKa17i/HGAQhLywDgiIAJhXasr8r13cjkVujKFcU0I+BBuAvSqm1SqkbyjQOQciNUS0hRWmteLS2vyvlCAAR2qMoVxTQQeU4ryAUjJmUPIDJhrZAKdEAxqASooCEyYplQde26l8NawvQEgU02XAFgHIMHeIDGIUIAKFwHvsO/PVb5R5F6YjU2r9FAEwSXBOQKwDkOxuJCAChcHr3QqQB3vbdco/EewJBWHru0N+ymqx8xAk8LiIAhMLRlr0qXvEP5R5JaRENYHKgLUClOYFFAIxEqoEKheOusPyGCIDJwSgNQL6zkfjw6RWKhq8FQJU7vqsB9/4UDSArPnx6haKhNaDKPYrSI2GgkwM3DyDgTHPynY1CBIBQOL7WAMScUPGMSgQTATASHz69QtFw46z9hgiAycGoPAD5zkYiAkAoHL9qABIGOjmQMNBx8eHTKxQNvwoA0QAmBykNQExA2fDh0ysUDREAQkWjJQx0HHz49ApFw9cCQMJAK55UGKjUAsqGD59eoWj4WQCIPbnycTOBxQeQFR8+vULR8LMAEBNQ5SOJYOPiw6dXKBpunLXfEAEwOdBSDXQ8fPj0CkXDr3kAEgZa+WjNaCewfGcjEQEgFI6YgIRKxXXSS0vIMfHh0ysUDzEBCRWK+/0oJbWAxsCHT69QNPysAYCEglYy6QJAnMBZ8eHTKxQN3woAsSlXPmkmIPEBZMWHT69QNHwrABzHt5iBKpeUBiBhoGPhw6dXKBp+jQKSsMLKJ/XdSDXQsRABIBSOX/MAArKirHjSNQCpBZQVHz69QtHwrQlIVpQVzzATkNQCyoYPn16haIgJqLzjELKjxQmcCyIAhMIRDaC84xCyI07gnPDh0ysUDd8KALEpVzwpDUCqgY6FD59eoWj4VgBIGGjFIz6AnPDh0ysUDd8KADEBVTwZM4Elc3skPnx6haIhAqC84xDGQJzAueDDp1coGn4VAJIHUPkMMwEpQMn3lQEfPr1C0fBrIphoAJVPeiYw2EJbNIBRhMo9AGES4/c8gNvfC6GId+d4y5fhwLd4c/xqJ10DANsPIBrAKMoiAJRS3wQuBCxgL/BBrfXOcoxFmAB+NQEtPAmWXQBmwrtzvL4aNj8iAqBQ0hPBQDSALJRLA/iu1vorAEqpTwJfBT5aprEIheJXAdCyEN57m7fn+NZ8mbAmwigNICBRQBkoy9Orte5Oe1lHymUvTCr8KgBKQSAIllHuUUxexjEB3f1CK0u/fD+/+vuW0o+tgijb06uUuk4ptR24HFsDyLbflUqpNUqpNW1tbaUboDA+IgC8IxASATAR0jOBwW4LmaZRrd3WScKweOqN9jIMrnLw7OlVSq1WSm3I8HMhgNb6S1rr+cBtwFXZjqO1vlFrvUprvWr69OleDVcoBK1JRVkIxUUEwMQYRwPoHrT/t3u6B0s9sorCMx+A1vqsHHe9DfgT8DWvxiJ4hGgA3hEIiQ9gQozUAIY7gbsGkgDsFgFQepRSS7TWrzkvLwReKcc4xmIwafKvd66ls9++Ud52xCzed8Ki8g6q0vBrHkApEB/AxBhPA3AEwN7uOFprlB/DmSlfFND1SqmDscNAt1KBEUBb9vdx/4bdHDSjnraeOP0JQwTASEQD8A4xAU2MjIlgQ4l73YO2AEiYFnt74sxsjJV4gJVBuaKA3qm1PlxrvVxr/Xat9Y5yjGMsDNNWIa8592COWdRC0pRApVH4NRGsFIgAGJNt+/v51B0vcM/aLFPHKA1ADcvc7h4waKkNA/DhX65JbV+zpZ0eRzj4AVm+ZcGw7Ak/FFSEgwEMqf0+GtEAvEMEwJj8fu0O7lm7k6/e8xKGmeHZHMcE1DWQ5MIVc2mqCdPeZyf09cYNLr3h7/zLbc97PfyKQZ7eLLg3VSgQIBQMiAaQCREA3iE+gDFZ19oF2BP5k5v3j95hjEzghGExkDSZWhfhkpVzU36+waT9/vodXd4OvoKQWkBZSGkAAUU4oEhmWmX4HREA3iEawDC01nz9Dxtp7egH4O+b93PuYTP5y8Y9rNnawalLR4SIj9AAtArS1t3P39fuoD9hT/SNNWE09so/YVgkDPszfjJqigDIgusDCAUDhIMBEQCZEAHgHSIAhrG/L8EtT25hTlOMlroIi6fX84/HL2TLvn7Wbu9kMGkSUIpIyK3UOjwMNGHB81v286nX1qaOOa+lhoAz23cNJIkb/nvGRQBkIenY/ENBRSioUgJBSEMEgHdIHsAwXDv9F952CG8/ck5q+/J5O/ntc60s+8qfCSi45Z+OtbWBERqAoQMEsfjBu4/kyPnNRIIB5k+p5d4X7RqUnf2JlNbvp5BQEQBZMM00E5BoAJmRPADvCATB9E80ynjs77UFwNS64eW3P3nmEhZPr8e0LL734CY27OwaIQDsydwkwNGBV4ms+TD164emvVP7k9wW7mbG3T8moOC2cBdhU8EvflTYQEM1cP73oXl+YZ8vMfL0ZsGN+gkFAoSDihPN5yDRX+ZRVRiiAXiHmICG4WoAU+qHC4D5U2r52OkHctVbltAQC7G3O+68M9wJvGXO+byu5xLShi1YnZ8wBmFlYBlxtJkgrAzCDN8n55/BLnjtAWh9plT/lgkjGkAWXHUwHFTMir/Jl4Lfhj9uh4t/WuaRVRCSB+AdIgCG0d5nT+xT6rI34JnVGGN3l1PaYYQJaO28y/nqcytYc/lZxOqjqc90dPTz7m8/QnN7mB++ZwX/9PNnmVYfZc0VuVaySR/kG/Cjo8DwsE9EkZHlWxZcm38woKjTfQDo9s3lHFLlIRqAd4gAGMZ+RwNoqc0uAGY2xtjTM0IAODE9PU7xt/ro8DXvnKYammrCdPYnU2amggk52cTG5KkvJBpAFlybfzgYIOSscqX25QhEAHiHD53A3/jDRjbt6Rm27eBZDZx44FQ27OiiuTZMOJj9fpvRGOXJ13vp6k/SNEID6Bk0iAQDxMLBYZ8JBBRffNsyrv3detp6bC2jYKU26GgWXnaKKzIiALJgWkMaQMi5Z6Sh0AhEAHiHzzSA7sEkNz/xJgum1DK9wZ5I44bJTX97k5v+9iYAR8xtGvMY85pr2N09yFHffJAn3hNiNqTuz954kvpY5umuIWaXhNjX6wiAQi8i5AgA0QAmP8m0UhBBVwMQATAc8QF4h88EwJtttpn1S+cfwrmHzUpt37q/L5WpO39K7ZjH+KeTDkAD/+/h19nbPTBcAAwao8w/Lu52VwMomJQAmDwagCzfsmC6JiCnFARI38pRiAbgHT4TAG/s6wXgwOl1w7YvnFrHkfObOXJ+85gOYICWuggXrrBzBPrdgm4pDSC7AGiIDRcABa9pAiH7fJNIA5CnNwtuFFAwqEglF5ZxPBWH1oDkAXjGiAYm1c6bbX0E1Pir/PForLHNOf2Dzircmc27B43URD+S0SagAiWAUrYjWATA5CcVBhoIDAkAsQENMbLYllBcAiFfJYLt7BpkZmOMaCg4/s5j0OQKgIQrAOz7sy9uUJdFA2h0NQBHALjPfkEEI5PKCSxPbxbcaqDBgCLsLAi0xAClIQLAU3xmAuroS4xr4smFaChITThInxP26WoAg0mTmnBm4eJqAK6vIWFMQPMSDaA6cMs/h4OKkLJviIksDKqOEan2QpHxWRjo/iIJALC1gP7EcB/AYNIiGs483cXCAUKBofs4MZGyL6GoOIGrAdPSBAMKpRQh7Bti6/4+1rf6p1b4mIxsuCEUF5/1A+joT4yZ5JUPzbVhBuKu+Wx8DUApNcw/EDesws29oahoANVA0rIIOquCSMBeiXUPGtz61NZyDqtyEAHgLT4zAbUXUQNorAnT1e+EdKY0AHNUElg681qGnM9aU3gDqFB0UvkAJA8gC6apCTsCIJTWSi4+EftgNSECwFt8JACSpkXPoFE0ATC9Icq2rX0QsfsAhLVm0LCIZTEBAfz6yuPZ3TXIH9ft4oerNxE3zKHeAvkQnFwawJgCQCl19YhNGtgH/E1r/aZno6oADMcEBBBUQ5P+hOyD1YQIAG8JhOwetlpXvZ+lo9+p81MkAfDVCw7lwYHHYTsMJDWYGtPSxMaIMKqPhjhoRj1T6myHcNywaCjk5KEYGBNMKCsh4z29DSN+GoFVwP1Kqfd6PLayYlhWqu5I2PEBaFSqbZzvEQHgLQFnbeYDR/C1d60DYEqRfAAzG2McMK0GgLipGXS09prI+CGmbhhqwd3BQtFJJQDG1AC01l/PtF0pNQVYDdzhxaAqAcPUhIKOCYh0E5AIAEAEgNcEnMnKMiBY3ZbaN/bZZSBOWTqtaMeMOLdl3NDgNHuPjuEDSH3OMfvEkwUK3moSANnQWrerKu+bljQ1oYB9MwwzAYkAsJFEMG9JaQDV7wfo7E/ygRMW0ujE4xcDVwAkLCBpP7OxHGz6UVcATEQDMKtcACilzgA6ijyWisK0rDQNwDEBaSU+ABfRALzFJwLAtDTdg0maimT+cYk4z27c0JjOan6sKCAXN1egcAEwuRLBxnMCr2d0CZwpwE7g/V4NqhJIWjqVHKIsO6ZYIxpACo8TwXZ0DvDjh18jYQzdfmcfOoPzDp/tyfkqDp/4ALoGkmgNLbXFW/0DuHP9oAkhRwBkywNIJ+UDKNQEFIxAog92PF/Y52NNMPXAwj5bAONpABeMeK2B/Vo7LbKqGDPNBGQk7VWYOIHT8FgDePiVvfz6me3MbooRUIr9fXE27enxkQBI8wFUMZ3943f6KoSQsy6JmxrDNQHlJAAmqAHUNEP/fvifMwr7PMCn1kHLwsI/nwfjOYF9m/VkpJmAZjXY/6ammrCYgFxGtNwrNknnAfzTJ0+hpS7CNXe9yKOb2jw5V0WS0gCquyBch1N/p6nIGoAb8JMwLFTKBJSLDyCY+lxBnPJZWHhSYc1DdqyBx74LAx2VIQD8jO0Etie3gPMQRsMB0QBcPNYADMs+viuEpzdE2debSJXoqHpcAbDlb1A/o/jHD9XA/GOHNI0y4ZUG4M71cRN0KX0AsUZYem5hn3XNqSU0+4kAyIJp6VQjGPcLUUpMQCk8FgBDxfjs489oiGFamva+RKplYFVT02L/vvsj3p3jst/C0nO8O/4IugeT/OTh1xlIs6+/6YSAFt8H4NQAMnR+AiBlAiqD76UMZj8RAFlImtZQhUDnCwkqLQLAxWsNwBEA7ncww5n09/YM+kMALD0PrvyrNzHlffvgzsuhZ1fxjz0Gz7zRzs8ee4OGWGhY9c1lsxqY2Rgr6rlCyr5/bnhsC7Pm2efKxwRUlnwf5TYfFw2gLHT0JfjnX66hZzDJtvZ+jl7orMIcE1AQi7j4AGxKZAJyzT3upP/gS3vY3j6Q2q8uGuSkA6cRqDazUCAAc47y5thxu/0ig53eHD/baZ1J9a6PnsjBswoqtJAz7lzf1ptg06Y25rXUMK1+/IVDdKKJYBOhDKG/IgDSeGNfH89t7eCYRS2ccfAM3n6k3V/U/UJCmCScUrFVngc3Ph4ngiVNTTioUv/nBVNqCSj4r4deG7XvXR89gVWLpngyjqokUmevNgc6S3rapLN4KqjIWp6EA/b9qQmw+urTWDStbpxP2EzYBzARRACUF8uZ1D515lJOXpKWlu74AIJOSYikqYmE/C4AvDYBWakwXIAZjTEev/YtdPUPRcVsa+/jo7c+z+7uyZN4UxEoZYcrllgDcM2nrn3eS1xrv4ViRmPuJsNIsJwCwPUBlO7cIgDSMJ2WX4GRc5rTmzXoTHoJ0yrJKqai8TgRzLCGajG5zG2uYW5zTer1tAY7cqS9b/LUX68YYs0l1wDiJdQA3PxVC0VtJPdpLhQMEAwo3ziByzqLKaU+o5TSSqniVYGaAK4ACI6c1JwvJOBoAOIIpgQmoKFqrNlwQwdFABRATTMMlra7nZvbEQ2WIPTUWaBYBeSpREMB4kl/mIDKJgCUUvOBc4Bt5RrDSFwBMHLlmYoCEgEwRAmigELjOHbDwQBNNWERAIUQay69CcjRAMKlMJ9OIFExGgqU1wfgkyigHwLXAPeUcQzDMJ1VbSCbBqBFAKTwPApIj6sBAEypi7BfBED+1DRD+xtDEUElQMd7qWWQiNkPcY/Xnkk7UuywuS15fzQaCpbHBKR8kgeglLoQ2KG1fnG8aBql1JXAlQALFizwdFyWawIaufIcKQDMMhbouvWdsHNt+c7v4pYo8CiTNL0Ux1hMqYvQIQIgf2qnQseb8K25JTvlx4CPxYBvl+yU3HrliXl/pmwZ/2UoAOiZAFBKrQZmZXjrS8AXsc0/46K1vhG4EWDVqlUFdmrODcPKogE4TmBXALy8q4cDptWXpyTB5kdg1uEwd1Xpzz2ScA0szP8By4VcTEBg+wH+vnkfV9+5lu++60h/lIkoBid+ApoXFFazpkAeemUva7a0c+15y0pzwuYFBKO5hX+mEwoofr92J1e95SAOmuFtvsIwUk7gKhAAWuuzMm1XSh0BHAC4q/95wPNKqWO11ru9Gk8uWDn6AD7x6xcYSJi8+5j5JR0fZtK2Dy67AE67prTnLjG5OIEBLj5qLm+09fJ/L+zgs+cezJy0KCFhDJoX2EKghDy+/yV+t72Va08qsFZOibhoxVy+/5dNbG7rK5MAqGInsNZ6vdZ6htZ6kdZ6EdAKrCzl5D+QMLnliTdTE76L6wMYHQVkT/xhZXHbPx8HUB67s2PXJFz9k1ymMNBMnL98Np8992DArjUjeMu61k4e3dTGC9vy7weVNK1UnH0l89Yj7JLjJXcESyKYR7z+EOx6MfVy+95e9jy/g7auhcxsGKpBsnhXNx8L7mLKC+uhLq064X47+1RZBicsngqUqViUKwBCxa2bUokkRySCjYXbSjA9SUwoPjs7B3jHj59IvX74M6exeHp9zp9PGJMjf8YtB1FyP4AfBYCjBXjLq3+CZ/839XIpcG0YeGb4bocCh4aBp7IcxzIJBBSRYJnCxAwfaQBOKYhcaKqxBUD3YHU3Tyk3roZ13mGz+PNLu9nXm2Dx9Nw/P1kSKMtWETQVBiqZwMXlvOvhnOtSL/+wbief/e2L/OpDx3FsWg2Z3z3fyhfvXs/qq09jfkvt8GM88u/wjC1EJpQoYiTg7z+GRAHhdy2L7N8+0AAMKw8NoMa+jbsGRAPwErdC69KZ9fz5JejJ0+SWq1+n3ETKpQG4IdV+0gBKQjBs/zgYgQhxIiSIQDg2answUjNsOwCBsB36aFnEQpBIJgur2bH+N/DQ1+2Y33zKKFiGHboHvtAAkqYmFs5PAxAB4C1uIleLYx7tyVPjShiTwwdQtpLQfjQBlQO3ovPIeH53e8ZQwlDU/mK+0cKzAOucn0KYthQ+/kx+AuBXl8Dmh5yx+EMDyHW12OD4ALpFAHiKqwFMcQVAPL+JKj5JfABl0wCqKQ+gkjGdlXvC0Bm3j8oDADjqH+0wLcvi50++yYyGGOcfUWCD8iVn519ELToUjmYEoylpFZoEK6pCyDUPAGyB3RANiQbgMW45Z7cGUyEmoMmgAQQDilA5CsJVUx5AJeOu9JMjmrukagFlmnia5sGpnwPgzrWPsaCplvNPL2EyVqwx9efFN77Aem0X8vri25Zx5akHlm4cJSJfe3FjTZiNu7r5w4s7OXXp9JRZyAu27u/ja/e+NGyFGAoG+Mr5h7BkZgnjxkuMawKqdzp6FWICyqcyZzkpS0E4v5SCKDduvP9IFc/RcMftLhUNBVIPQ8mIDgmA0w5bwDmzl3LTE2+ycWd3acdRInLNA3CZP6WGp95o55k32/n0WUv49FlLPRvb02+289dX21g+r4loKIBpaZ7fvJ9vAisXNBNUinetms+spuoy1bkmoEgwQEMsRG++AsC0aCpBL4BiECnHMx4I2I5gnxSDKxum88WO1ACy1gIaQTQULPnqoMuqocn5+6pzjiA2YzH3b9id9ypssmCbgHLXAH7+wWPZ2TXAxT95wvPqoO5EeOP7VjGrKYZhWhz0pft5bFMbj21qA2BH5wDXv3O5p+MoNe7zEg4GaIiF8zcBGXpS+ACgPM84YPsBRAPwFnelP1IAGGOZgNKIhgP05ekAmyj9akgAxGrs+iYNsVDVCgDbBJT7arEmEuTA6fU01YY9/5+4/YpdDSUUDFAXCdKXMLny1MV09ie4c812/vDiTgCmNUT50ydPoS6a++P27JZ2Lv+fp0etQhdOreXhz5xelppHQwJAUR/N/96z8wBK0AugCJRFAwDbDCQCwFvclX7CHO4EtrKVgx5BNBSgva+0N4cRSbMtOyGqjTVhtrf3l3QcpSJfE5BLfdR7AZB07ptwmoYSDgUgYdJcG+YDJy6ipTaCaWm27O9n9ct72N7Rz7JZjdkOOYqXd3WTMC0+ctpiYs6kuX5HFw+/speugWQqEqeUpK7bMQGt3d7Jx297nsuPW8CJB43f0ylh5CfUy4ndE6BMjeGlJaS3uCv9UT6AfExAJQ4RS4bSBEDIzgOodg0gHxOQS0MsRG/c22ggI0NjE7eoZnNNhLnNNXzhbYcA8OTmfax+eU/eZqmOPvsaPnvOwSln+D1rd/DwK3vp6E+USQAMmYDOOWwW+/u28eDG3URCgZwEQNwwU1m2lU4kVK6S0KXVACbHt1Fk3JV+tiig8bTrcqwOkqG0srZOUltjLFy1BdDyKQWRTkMBpol8ce+bdAHlapXNtcOjj6bW2Q3JXQEwmDR5fW8vnf1jC4SO/gQN0dCwSKhmJ/xyvM96hWEOmb4+dPIBrL76NBZPq6c/Mf7/27Q0Hf3l0VwKoXxdwcQE5DnuRJ9JAAQDivGa1ETKECKWDKUV3XLGZ692DSxLjxu5NNmwG8IUpgG83lYiE1CagHIXFc0jwk9b6uzXrgD49B1r+fNLu2mpDfP8V87Oeq919idorhtxLEe4uNpBqUmkmYBcaqNB+hPjL4Y6+hOYlmZGw+SIjCrHMw6IE7gUpExAIwWA1qNLQWegHKuD3qYl/NE8liOPPJp5zraGWAitoS9h0BCze+Oua+3k+MVTiYUrz9lmWpoXWztzUq2TpiZcgFCrL4FZzLCsUQsFt7J4U+3ISdte8e7vtQXAi62dAHT0J4kbVtbvqaM/mfrsyGN1lFkDSE/mqo0EGchBAOztjgMwvSHqzeCKTDQUpLMciYWBkISBeo2VxQdgr6TH/3w0XPqeoUagho8nP81vV52QJgDsyaZn0BYAV/9mLX99tY2vXHAoHzr5gJKOLxf+vGE3H7/9+Zz3bywgmas+Gs47Pj1fMpmn3NyS5hGTttu4vqM/Qc9gkl1dg8xsjLKnO05/wswqADr7E6OO5ZqXOstU9jppDo9+AqgJh2jvGxj3s229tgCYMUkEgK0BlMMJHJRMYK8xx/AB5KoBJAwLrfW45qJiYWQoU9EQs7++z/zmRWojQR51YtArtSRC54C9cv3vy1eOspWPJKgUKxY0532OhliIhGkxmMw+uU6UhGkNiwACO3Q4ARkzkKfWRVi7vZMbHt0MwJHzmnlw4x764kZWm3hHf5JF04a3M6yP2hm47WXSAFzTV3qYtK0BjC9w23ommwYgYaBVS8oHMKIWkOH4AMYjGgpgaXv/UoW1uTbm9PGtmN/MqoUtdA8m6R5McvSCFtZs7ShP9EIOuAlUxy+e6pkz0BWKD728l/OXF1iraRwMc3SI6h1XHs+9a3dSFxktdJbMrOeBl/awrrWLSDDAcYun8uDGPWPazjv6E6NMQEopmmvD/ObZ7Tzx+r5h702rj/LTf1yZqmTpBW5uRvqipzYy2gfw2KY2tu7vA2xt+U/rd7Fln/16sgiASChAz6DBM2+2Ew0FOGJuU2n8bIGQaABeY2bxAVg6VwEwVC62VPXNU5VK0x6+eS213PWx4U3Zj/jaA+WJX86BTCaEYnPobDvW/udPvOmdAMjgoF4+r5nl85oz7v+Ty1am7Mk14SDPbGkHbN9NJixL0xs3MprAPnzKYp56Y/+wbXt74jz8yl5aOwY4MI8OXfmSqT5TzQgfgGVprrjl2ZSfzWXlgmZOPGjapKkF1FIboa0nzrt/9ncAbvrAKs48ZKb3JxYnsPdkEwBmrhpA2OkYlDSpzyO7cyKkQlTHkTfRcJnC13IgUwJVsVm1aApnLpvBrq5Bz86RNHVeVS1DwQDT6odWvnXOJNgfzyyoe+IGWkNjbPS99ZHTDuQjpw0v/vfwK3u44pY1nvs+kqYeJQBqI0H6k2bKHJowLQxLc9UZB1ETCfLdB14F4LcfPbEs2cuFcvXZSzlz2QzaeuN86o61pesBLj4A7xkyAY3WAMbLAob0lnGlm2iHKpWOPfFEQ8EKNgF5rwEA1EVDWVfXxSBpWhO6hlrHTJQtft6tsdOQQQBkoj46FAzgJZnKc9RGQpiWTkU0uYuq5towC6YMddWbTJM/2PfQiQdNY5/jvB4slUM4ECxpFJAvE8GyOYFzrUEfKYcASPkAxt4vUq4ElhxI5lhraaLUx0Ke1mrKp1dBJtyaQNl8AO5E7ja7Hw9XC/U+A3q0BlDjONpdM5C7+IiEAsxtmfyd69xAgtIJgNKagHwpAKwx8gBycfQM+QBKJ6mtVJby+HWKyhK+lgNGBieiFxRSqCwfJtrb1nUUZ9NS3LE35CgAXE2hFBrASM2nLupoM849l0zLFZjXPKKv9iQk5iz2BkuVFCYCwHuMLFFAVh5RQEBJMwVzr1NUpvC1HCi0vk++1EdDxA1rlIZXLAotVOdS45qAsvkA8jQBufv1elyhNpHRCWyf2w0FdTWAcDBAY4393rtXzWOyEgoGCAcVA6VaVKnS+gB8KQCyawDkmAcQzPh5LzFzrFRathT2HEhmCJ/0AtfE4pUZaKIagBsJk00D6M5TALjX63kGdAbnd61jIrn96e1AmgYQCqCU4tV/P49vXTK5+yLEQsHS+gBEAHhL9kQwK88ooDI4gceZQO1KpRVqAsqj0ftEaIh6uyJOZkgEy4dgQBELB7KWUMjXBBQOBoiFA55rAJlMQAfPsqvUPrhxNzDkF3O/52goOOkcwCOJhoMlNgEl7ZLQI3+0Hv/zeeLLKKCxykHnZQIq4USbMgHloAF0DlSmBjBR52mu1HksAAxz4p2t6iIhfvXUVu5ZazeNef+JC/mX0w8C0gVA7o+n3aFr6Hp3dA5w15pWTG3/z9977PwJF2JLWqOdwPOn1HL5cQv48wZbALjP1GQp+5wLNZFA6TSAUBTeeAS+0TL6vct/B0vOKu7pinq0SYJrAhqtAYxvYoEhE1BrxwC/f2HHqPfDwQBnHjKjqKUIUs1qcvABVLIJqBQaQH3MYxOQpamd4HV87tyDeWFbJ2A3evnvR+wyEQrF46+1EQkF8rp/GqKhYQLvzme386OHXku9DgVVSsCks7mtl+e2dOR0jh0d/any1unUx0L0OOdOZqgYOtkpqQno9M/DnJWZ35tS/PpevhQAKSdwho5g+WgA1/3p5awx999553Lefcz8CY50iFw1gLLVMc+BfNs8Fkq9E5nilU3cMC0iE7yO9x67gPceuwCADTu6uOS/n+Q7f3419f7yeU3ZPpqR+liI1Rv38PHbn+cnl62kd9CgPhpiw9fPZcU3HmRXZ+bEuM/85kXWbu/M+TyHzRk9roZoiIRhETfMYWGg1UIsXEIBMOco+6dE+FIAuBrAyIky11pA7s2dMCwOmlHP/7x/Veo9rTUX/vgJ1u3o9EYAjDPxlK2TUQ4UWuM/X9zEqL4sUTYTpdjRTIfPbeKlb5yb+o6BvDKNAa48dTH/76HXeWDDbrTWDCSNVMLZrMZYxszogYTJhh1dfPDERXz41MU5nWdmhlo+rq+id9AY5gSuFmpK6QMoMb4UANmcwPmGgQJMq49wwIiqjYfMaWRdaxdd/clR9eELJXcNoHKdwMmS+QCcOHsPfQDFjmYKBwNMxGJ4wfI5bN3fz3cfeJW4YdEXN1MCYHZTjN3dw0s2P/n6Pn7y19cxLM0pS6Yxt7nwpK36NJ/LkBN4cjt+04mGA1XberV6xHQejNkRLBcfQNqTmilb8/A5Taxr7WLlv/+laE3bzQzVQDOOrYJNQMYEwydzxZ2QejzzAZSuCGA+pOcD9CfMVLjprKaaUSag7z34Kuu2d3HC4qkct3jqhM5bn5aI5j5T1eQELqkJqMT4UwPIJgB0jg1h0m7uTBUbP37GgURCAW54dDOtHQPMnzLxjMhcM4Er2wRUmvLZXucBFNqv2GvcInN9cYP+xJAJaE5TjP19CZZ++f7UvgnD4trzlvGx0w/MeKx8SA+7TYwIA60GasLBil1UTRSfCwA9rJ+uaWkiOejhoYAioOw2gBkbgNRHuWD5bG54dHPRmrYbOWcCBzEsjWGWxt6eDwmjNGMKBwNEQ97FxScr8H8Lw8Nf+xNmanHyrlXziRvWsBLNkVCAy45bUJTzuhpAtfoAxsrZmOz4QgD8cd0u1mxt57JjF7BkZsMwZ1vSsogG7EnfzLG5ulKKUMAuuZCtYJcrGLqL1J1rSAMYe7+Ug7oCJynD0sTCpRlTQyzkoQAorF+x19SnNB+T/oTBrEY77n9WU4zPnnuw5+ftjRup7Phq0gBi4SDdg0nuWbuDdxw5p2RdAEuBLwTAi62d/PyJLfTHTb596fKUPR3sh9kt6W9pTa6avXuju/VORuIKhu4iOY9MJ0R1vJvPNU+97b8eJxBQTK2L8MsrjkvVnyknhmkRKlH/hLpoyLP6+JWoXcFw53d/wqQ2WprvPOUDSDMBVZMGsGBKLf0Jk0/dsZZDZjeydGZDuYdUNMoiAJRS/wZ8GGhzNn1Ra/0nr873xbcdwsad3by0qwsYWk2D0xPAiWyzM4Hzu3GzaQANsRBKFU8DMK3c6hSdecgM1u/oImla7Owc4NktHezs8rZT1EiefmM/R85vHpXIVKpEMLBXpcX0Abyyu5tv3reRpKnpT5olqWmUL+nO7/6EmfIJeI37DHT2JVJhyvmGsVYy/3zKYua11PDRW58v2vNcKZTzW/qh1nqF8+PZ5O9y2NxGNu3u5dantg4L6Uov6GYLgPyOm8kJDHbGbn00VLQG7VaODuqFU+v44XtW8OPLVnKlE9tdyszgjr4E77nxKS76yRNsbutN/SSc6pylcp7WRYtrAnpq836eeH0/lqU5YfFUzlxWgvaAeZLu/O6LDzmBvSYWDtJcG2ZPz+CQBlBFAgBgulNGw+t6S6XGFyYggBMPnMbPHn2DL/9+w7Dt6REzHf2JnDMwP3jiIp5+s51D5zRm3acxFi6eE9jUeScflaNvgfuAvLK7hzO//2hq+7tXzXPKKJfIBxANsbu7eG0hXQfqzf90TM6NWkqNa4rpHkgSN6yS9t+d1Rhjd1ecppqwHSRRgT6SiZDuX6kmyikArlJKvR9YA3xGa52xIIlS6krgSoAFCwqPWjht6XSe/uKZHPcfDw3b7kYtJE2LvT1xZjXllhDzb+84bNx9GmvCdA8UZ8Vgt6vM7zPlaF3palRnHzqTC5ym7P/9yGa27Ot3qmiWTgMopgloqCVn5U5srsnHbWNYKg0AYGZjjD3dgxwwrbaqHMAuuSYX3r9+F9978FU0cNGKuXzyzCUlGF3hePZNKaVWK6U2ZPi5EPgpcCCwAtgFfD/bcbTWN2qtV2mtV02fPn1CY5rREE2FUbq/3Qlrb08cre2Y6WLRGAvxxr5e/vbavtS2waTJF/5vPVfd/jxX3f4896/fldOxcq1Umo6bsFZKAeCam965ci4XrrB/DppZz76+uCcZtNkotgko1zDcchIMKOoiQf73b28CQyahUjCzMcqe7kGSRaiUWonU51hh9onN+9jeMcBAwkxVSK1kPLtDtNY51S1VSv0PcJ9X4xhxLppqwrT3JYiFAvQlzFRXsF2ddqr8rCIKgAOm1fH0m+184OfPsPEb5xINBXlpZxe/fmYbs5tidPQnaOuJ89YjZo97LDPHQnXpDHUuK53amsgQBz6tLsK+njiRUKBkq8Nih4EOaQCVPbld/87lvLyrm3AwwHmHzyrZeWc1xmjrjfOXjXuqVAPILbkwnrSYVhfhpIOm8bfX9425byVQriig2Vprd+l7MbBhrP2LSbMrAMJB+hJmasJyi2XNmUBNlJFcd/ERzGup4XsPbqJ30CBaH6TXsSH++LKj+NFDr9PZn8jpWKapcypVnY4bc19SE1DKCThkfphWH6V70KAmHCzZ5FAXCTGYtIoWsmnkmIdRbt5+5BzefuSckp/39GUz+OumNgxTc/wES0tUIuFggIizaByLuGERDQdpqYvQ3pfbs11OyuUD+I5SagWggS3AR0p1Yrc4mxui6PoA9jgOwxkZqh0WSjCgUj6F/oTJVKDfWUHURUPUhIPszjFCx23skQ9DTuASmoAch3M0LeFrar39Px1ImiWzoQ/1BDBpqi2CADAtQjnkYfiVlQtauPeqk8s9DE+piwTH1wAMk0gwQEtthLhhMZAwKyIHJxtlEQBa6/eV47wALbURYKgxt7ti7c6zDV+u1Dnncc0R7u+6SIiaSDDnZtNWjlnK6ZSjc1mmMMCp9ZHU36WKAkr1BIgXpyJrIT4YobrIJbAgYVhEwwFanHuuvT/B3EjxrArFpvqMdePQ7MTt14zQAHoGk9RHQ0V/yEfaDvsdFbIuGrJrjOQoAArzATgawAgto3swyZd/v57+LE3JJ0KmTFC3JAEMCUSvKXZPACNDO0TBX9TnEFgQNyyioQAtdfaip6PCzUC+yQNwaXY1gFECwMirB2uupASAM/G7N1BtJGiXmc2xyFSuparTiWbxAdz46Bvc+tQ2Fkyp5cpTJ14NMh33XOkVU5fPa+KGfzyagaTBWw4uTQKVG7ZXLEewaABCXTTEzq4Bnsjg3F00rY65zTXEDYtYOMAUVwDk6OMrF74TAO89dj6mZXH0oik8s6WdhNMWsmcw6ZEAGB4/3J8wCAYU0VCAmnDuJqBCJiDXDDPSBDTUEEeP+sxEyaQBKKVKGpECw2vjFwPDsio6B0DwnhkNUe7fsJvL//fpUe8tnl7Hw585nbhh0lQTTpmar7r9BV782jmlHmrO+E4ALJ3ZwNcvPJxt++1GLe6E1Rs3UrG+xcRNznEnor64SV0kiFKKmrBdujmZQ6OUQgRAIKCIBAOj2tm5h9G6+AIgXiHlgIvdE0A0AOH6dy7nn04a3Zj9F09u4bFNdlmzhGMCWjytjilOJFDcMFPm2ErDt0ZNd4L6+r0vobV2TEDFT/F3hUp/SgAYqcnJdUTn0m3IzgTOfwKyO4QNP75rSrKKP/+ncg6iwfLe8KnEnSJVBDVK1M5SqFyaasIce8CUUT8HTq+jN2FgWTrlAwgEFP96lp0FXKx6YF7gOw3AZUZDlPpoKFU5sWfQYEEROneNZKQPoC8xJADcTN2BpDmu8Cl0BRoNj24RqVICwAMTkNsSsER1/7ORa+ZmrhiWTlW6FCqfZDJJa2srg4PFqweVjZOnGxzx9tm8/MrLfO2UZmLhAC+//DKH1hr8zztms3vrZvaXKIAgFosxb948wuHcFrO+FQCBgOILb1vGl+7eQF/ccHwAxdcA7OxXNcoEBEOO6MHE+HH6RgFhoOA0iR9lAvJOA6iUapB1HgiASs8CFoZobW2loaGBRYsWeZ67sb8vzo6OAZbOakTt7aGpJsLclhp6BpO8ua+PA6fXl6Qsh9aa/fv309raygEHjDZVZcK3AgCG10/vHjRo9MAJDPZk1N6boL0vQddAMlWlsSZNAxgPq4BEMHA1gOHH99QHYFgVUQ3SbQt557PbeXKzHbVxxUkHcM5hhTmjTXECTyoGBwdLMvnDkEnV1BpLg3vK1HYvVloZUEoxdepU2traxt/ZwddLGlcA/Nfq10gYlidRQGDbDu9cs52V3/wLa7d3ptpF1kTsf38uAqCQMFBwNIBRJiD7tycmIMcGWgm8/4SFzG2pwdKwdnsnf8yx8F4mDFOcwJONUmVtp/cU13pogeXeL6YHz1k28r1m0QCAP79kV+27YLk3NVR+8O4j2bCjO/X6lCXTgKFyFLk0nLYscmoIM5JoKMDGnd3c8cw23nusXU7bDX01CggDfeCl3dz61FY+deYSVi2aMur9hGGVPQLI5UvnH5r6+/wfPT4hh7Bpla6SqTC5SNcANDo1CacLhkqlMp7UMuHWi0kYFqctnc6iaXWenOfohVP4wImLUj+LnfaMKR9ADqUaDMsqaAV6zKIW2nrjfPWel1LbXDt9LtFHI7njmW08/to+vv3nV3huazvPbW3n9b09w45dKQIgHdfhXyhGAe1CBX+QWuk7Cyp3EZ6ID3LFpeeTSBqsXbuWE044gcMOO4zly5dz5513jnvcH/zgBxx66KEsX76cM888k61btwLQ1tbGeeedV5SxiwbgMLUuMsae3uBqAP9+30Z+8vDrY+776u4eVi5syfscXzr/UJpqwnzvwU2pyXlIAORfJG5ru50/8eyWDt75078D9g3/0NWnsXh6fcXGPDfEQuzsLDwixLQkDFTIjBtUYVjWsNe3/PznnPXWt9MTt0gQ4pe//CVLlixh586dHH300Zx77rk0NzdnPe5RRx3FmjVrqK2t5ac//SnXXHMNd955J9OnT2f27Nk88cQTnHTSSRMauwgAh5YyCIADptVx/vLZOZWEXrGgmYuPmlvQedwIhP6EQSQUSTmFc9E80jEtTWv7AFecdABnHTIDw9J0Dya56vYX+OurbSyeXk/CrGQNoPB47KRZmAYmlJ+v/+ElNu7sHn/HPDh0TiNfe7vdFdBVDN2S8u5dctttt/H9n96EYVo0zVrIklkNAMyZM4cZM2bQ1tY2pgA444wzUn8ff/zx3HrrranXF110EbfddpsIgIlQn+b0nVImDeAnl630/Dzp2cjNtZG8TUAdfQlebO2kayBJwrQ4aEY9Jx40LfX+9x/cxDfu28i//3EjlobD52bvk1wu6mOhCfsAKlGwCeUnqBTBgErZ+pVSJBIJ3njjDU448hBaO/rpSbv3nnnmGRKJBAcemHsdrptuuom3vvWtqderVq3iy1/+8oTH7msBkG6qKIcJqFTUOvWI3EqkbrLWQI4moH/7w0vcs3Zn6vXBs+qHvX/9JUcM635UiQ1BGmJheuMGWuuCokMMS1Mr1UAnJe5K3SuUUhw8s4Hd3YO09yUwLIt9+9pTq/t04bBr1y7e97738Ytf/IJAjj6lW2+9lTVr1vDoo4+mts2YMYOdO3eO8anc8LUASKccGkCpcDUAty5O3hpAf5IlM+r59qXLqY0EOXhmw7D3j1s8leMqcNJPpz4aImlqp1pj/j4K8QEIYxEKBpjVGMMwNc01YfqMmlQWclApLK3p7Ori/PPP57rrruP444/P6birV6/muuuu49FHHyUaHWpWNTg4SE3NxPsM+H5Jc+B0O/LHqwigSqA2MkIDcARArr2C40mTlroIKxe0sGxW46TsijXR6qCGFIMTxiEUDLBoWh2RUJCWlhZM02RwcJBAQJFMJLjk4kt4//vfz6WXXjrsc1/4whe4++67Rx3vhRde4CMf+Qj33nsvM2bMGPbepk2bOPzwwyc+5gkfYZLzx0+eQl/cSLUtrEZGVsZ0TUCv7unh4v9+Yti+lx49j8uPWzhs26BhpZLXJivpxeGmFfBdSyawkC/nnHMOf/vb3zj6xFN54L67efzxx2hv388tt9wCwC233MKKFStYv34973jHO0Z9/nOf+xy9vb28613vAmDBggXce++9ADzyyCOcf/75Ex6j7wVALBwsyCQwmRgqSGcLADcz+JgRiVzrWru478VdowRAPGkSLWKv5HLgCoBrf7duWM2n6Q0Rvnnh4eO2qhQNQMiXj3/84/zwhz/k2JNO44JL3sOnP/qhVBmYdJLJJCeccMKo7atXr8567HvvvZd77rlnwmP0vQDwA27xObc9YsKwOHXpdH55xbHD9nvfTU+neiOnU6jdvJI4Yl4Tqxa20DNopCIyeuJJVr88wAdPPICDZzWM+XkpBy3ky8qVK+1QTm0vuLJlBD/wwAN5HbetrY2rr76alpb884JGIgLAB9Sm5QGAk62bYcVbGwmytzs+ans8aVZMfZ9Cmd1Uw10fO3HYtrXbO7noJ0+wrb1/XAFgSiawUABXXHFFqtaXVaSSENOnT+eiiy4qyrHkjvYBteHhGoCdrTv6q68JB+lPZtYAJrsAyITb/2Gbk908FoZlEZZaQEIBuLWCdnfHPanAOxFEA/ABgYCiNhLkrudaWbO1ndaOAZbPax61X00kxECG3gSDSXPSm4Ay0VIbpj4a4rU9PezrjTvbIhlt/dISUigUd+EQN0wsXVhVX68QAeATLjt2AWu3dxJPWhwxt4lzM9TFrwkHGUj4RwNQSrFwai13PLudO57dDsBZh8zkfz+watS+huQBCAWilGJeSw2tHQOYFlRSPqEIAJ/w5QsOHXef2kiQgaQ5LFvWMC0MS1elBgDw7Xcu54VtHQA8uHEPT27el3G1b5riAxAKp9TNYXJF7mghRU0kiKWH8gRgKGS0GjUAgMPnNvG+ExbxvhMWccnKufQnTO5bt5Nnt7Tz7JZ2tuzrAxwNQHwAQh4MDAxw2mmnYZomrdu38Z63nsZxx6zksMMO44Ybbhj385/73OdYtmwZy5cv5+KLL6azsxOA9evX88EPfrAoY6zOp1ooiJoMDWrcchHVqgGks2qhnRfxqTvW8q4b/s67bvg75/znY/QMJgvuxyD4l5tvvplLLrmEYDDI3Llz+NXvH+RvT63h6aef5vrrrx+3ls/ZZ5/Nhg0bWLduHUuXLuVb3/oWAEcccQStra1s27ZtwmMUE5CQoiYy1KO42dlW7RpAOvOn1HLfJ06ms98uG71hZxfX3/8KG3d2iw9gMnP/52H3+uIec9YR8Nbrx9zltttu4/bbbwegJholEk1gak0yHseyxi/EeM4556T+Pv7447nrrrtSr9/+9rdzxx13cM011xR4ATbV/1QLOTOyZhD4SwMA2yR08pJpnLxkGpc4/RfWbO1AawiJD0DIEbcc9KJFiwA7Em/3zlZOPvZo5s+fz7XXXsucObm3oL355ptHlYN+/PHHJzxO0QCEFJl6FPtJAxjJjMYYMxqifPeBVwGIhf33P6gKxlmpe8G+ffuGNXsJBhSz5szjkSefxeht56KLLuLSSy9l5syZ4x7ruuuuIxQKcfnll6e2STlooei4GkB6mWi/aQAj+ek/rmTDjm4CAcUFR8wu93CESUJNzVA5aLDbRAaUwtSaOXPmcPjhh/P444+Pqgw6mDTpHkwSCihqIyF+9ctf8Ic//IH7H/gLlgY3DqFY5aBFAAgpXCdwv2gAKY5eOIWjF04Zf0dBSCO9HHQsFqO1tZVEPExvOMCrW3fx6GOP8/4P/wt7uge56iMf4oorP8rKo49hf28i1Vv4iUdW871vf4ebfnsf23tMDogaqUKGUg5aKDquE/jzv1uX1kfYFgZRMX8IQl645aDPOussXn75ZT7x6X9Fa9Bac/mHP86U+Qexp3uQ9evWEaqfwp7uQUKBAEtm1GNpuOjfriUeT/CJ970TgBNOOJ4bf/YzQMpBCx6wZEYD/3DsAroGhjepP/mgaRw6u6lMoxKEyYlbDvqss87i7LPP5uUNoyORuru7OeLQZZy1aihR003CfGPz5ozHjcfjrFmzhv/8z/+c8BhFAAgpIqEA37rkiHIPQxCqArcctGmaBIPBjJ30mpqa+O1vf5vXcbdt28b1119PKDTx6btser1S6hNKqVeUUi8ppb5TrnEIgiB4xRVXXEEwWNwAiiVLlnD66acX5Vhl0QCUUmcAFwJHaq3jSqkZ431GEAQhH9JrWvmFfMtNl0sD+BhwvdY6DqC13lumcQiCUIXEYjH2799fcfX3vURrzf79+4nFYjl/plw+gKXAKUqp64BB4LNa62cz7aiUuhK4EuymyIIgCOMxb948WltbaWtrK/dQSkosFmPevHk57++ZAFBKrQZGF52HLznnnQIcDxwD/EYptVhnENda6xuBGwFWrVrlH3EuCELBhMNhDjjggHIPo+LxTABorc/K9p5S6mPA/zkT/jNKKQuYBvhLXAuCIJSRcvkAfg+cAaCUWgpEgH1lGosgCIIvKZcP4GbgZqXUBiABfCCT+UcQBEHwDjWZ5l2lVBuwtcCPT8N/WoZcsz+Qa/YHE7nmhVrr6SM3TioBMBGUUmu01qO7fVcxcs3+QK7ZH3hxzVLhSxAEwaeIABAEQfApfhIAN5Z7AGVArtkfyDX7g6Jfs298AIIgCMJw/KQBCIIgCGmIABAEQfApVS8AlFLnKaVeVUq9rpT6fLnHUyyUUjcrpfY6yXTutilKqb8opV5zfrc425VS6kfO/2CdUmpl+UZeOEqp+UqpR5RSG50+Ep9ytlftdSulYkqpZ5RSLzrX/HVn+wFKqaeda7tTKRVxtked16877y8q6wVMAKVUUCn1glLqPud1VV+zUmqLUmq9UmqtUmqNs83Te7uqBYBSKgj8BHgrcCjwD0qpQ8f+1KThFuC8Eds+DzyktV4CPOS8Bvv6lzg/VwI/LdEYi40BfEZrfSh2IcGPO99nNV93HHiL1vpIYAVwnlLqeODbwA+11gcBHcCHnP0/BHQ423/o7DdZ+RTwctprP1zzGVrrFWnx/t7e21rrqv0BTgAeSHv9BeAL5R5XEa9vEbAh7fWrwGzn79nAq87fPwP+IdN+k/kHuAc42y/XDdQCzwPHYWeEhpztqfsceAA4wfk75Oynyj32Aq51njPhvQW4D1A+uOYtwLQR2zy9t6taAwDmAtvTXrc626qVmVrrXc7fu4GZzt9V939w1PyjgKep8ut2TCFrgb3AX4DNQKfW2nB2Sb+u1DU773cBU0s64OLwn8A1gOW8nkr1X7MGHlRKPef0QQGP721pCl+laK21UqoqY3yVUvXA74BPa62709v+VeN1a61NYIVSqhm4G1hW3hF5i1LqAmCv1vo5pdTpZR5OKTlZa73DaZH7F6XUK+lvenFvV7sGsAOYn/Z6nrOtWtmjlJoN4Px2W21Wzf9BKRXGnvxv01r/n7O56q8bQGvdCTyCbf5oVkq5C7j060pds/N+E7C/tCOdMCcB71BKbQHuwDYD/RfVfc1orXc4v/diC/pj8fjernYB8CywxIkeiADvBe4t85i85F7gA87fH8C2kbvb3+9EDhwPdKWplZMGZS/1bwJe1lr/IO2tqr1updR0Z+WPUqoG2+fxMrYguNTZbeQ1u/+LS4GHtWMknixorb+gtZ6ntV6E/cw+rLW+nCq+ZqVUnVKqwf0bOAfYgNf3drkdHyVwrLwN2IRtN/1SucdTxOv6NbALSGLb/z6Ebfd8CHgNWA1McfZV2NFQm4H1wKpyj7/Aaz4Z2066Dljr/Lytmq8bWA684FzzBuCrzvbFwDPA68BvgaizPea8ft15f3G5r2GC1386cF+1X7NzbS86Py+5c5XX97aUghAEQfAp1W4CEgRBELIgAkAQBMGniAAQBEHwKSIABEEQfIoIAEEQBJ8iAkDwFUqpXuf3IqXUZUU+9hdHvH6ymMcXhGIjAkDwK4uAvARAWhZqNoYJAK31iXmOSRBKiggAwa9cD5zi1F7/V6fg2neVUs869dU/AqCUOl0p9bhS6l5go7Pt907Brpfcol1KqeuBGud4tznbXG1DOcfe4NR7f0/asf+qlLpLKfWKUuo2J9sZpdT1yu57sE4p9b2S/3cEXyDF4AS/8nngs1rrCwCcibxLa32MUioKPKGUetDZdyVwuNb6Tef1FVrrdqc0w7NKqd9prT+vlLpKa70iw7kuwa7lfyQwzfnMY857RwGHATuBJ4CTlFIvAxcDy7TW2i0FIQjFRjQAQbA5B7u2ylrsEtNTsZttADyTNvkDfFIp9SLwFHZBriWMzcnAr7XWptZ6D/AocEzasVu11hZ2aYtF2OWMB4GblFKXAP0TvDZByIgIAEGwUcAntN2NaYXW+gCttasB9KV2sssTn4XdgORI7Do9sQmcN572t4nd8MTArgR5F3AB8OcJHF8QsiICQPArPUBD2usHgI855aZRSi11qjKOpAm7/WC/UmoZdmtKl6T7+RE8DrzH8TNMB07FLlqWEaffQZPW+k/Av2KbjgSh6IgPQPAr6wDTMeXcgl1vfhHwvOOIbQMuyvC5PwMfdez0r2KbgVxuBNYppZ7Xdvlil7uxa/i/iF3N9Bqt9W5HgGSiAbhHKRXD1kyuLugKBWEcpBqoIAiCTxETkCAIgk8RASAIguBTRAAIgiD4FBEAgiAIPkUEgCAIgk8RASAIguBTRAAIgiD4lP8PSMkEgrz1znUAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph_utility_estimates(agent, sequential_decision_environment, 500, [(2,2), (3,2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ACTIVE REINFORCEMENT LEARNING\n",
    "\n",
    "Unlike Passive Reinforcement Learning in Active Reinforcement Learning we are not bound by a policy pi and we need to select our actions. In other words the agent needs to learn an optimal policy. The fundamental tradeoff the agent needs to face is that of exploration vs. exploitation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QLearning Agent\n",
    "\n",
    "The QLearningAgent class in the rl module implements the Agent Program described in **Fig 21.8** of the AIMA Book. In Q-Learning the agent learns an action-value function Q which gives the utility of taking a given action in a particular state. Q-Learning does not required a transition model and hence is a model free method. Let us look into the source before we see some usage examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32mclass\u001b[0m \u001b[0mQLearningAgent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m    [Figure 21.8]\u001b[0m\n",
      "\u001b[0;34m    An exploratory Q-learning agent. It avoids having to learn the transition\u001b[0m\n",
      "\u001b[0;34m    model because the Q-value of a state can be related directly to those of\u001b[0m\n",
      "\u001b[0;34m    its neighbors.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    import sys\u001b[0m\n",
      "\u001b[0;34m    from mdp import sequential_decision_environment\u001b[0m\n",
      "\u001b[0;34m    north = (0, 1)\u001b[0m\n",
      "\u001b[0;34m    south = (0,-1)\u001b[0m\n",
      "\u001b[0;34m    west = (-1, 0)\u001b[0m\n",
      "\u001b[0;34m    east = (1, 0)\u001b[0m\n",
      "\u001b[0;34m    policy = {(0, 2): east, (1, 2): east, (2, 2): east, (3, 2): None, (0, 1): north, (2, 1): north,\u001b[0m\n",
      "\u001b[0;34m              (3, 1): None, (0, 0): north, (1, 0): west, (2, 0): west, (3, 0): west,}\u001b[0m\n",
      "\u001b[0;34m    q_agent = QLearningAgent(sequential_decision_environment, Ne=5, Rplus=2, alpha=lambda n: 60./(59+n))\u001b[0m\n",
      "\u001b[0;34m    for i in range(200):\u001b[0m\n",
      "\u001b[0;34m        run_single_trial(q_agent,sequential_decision_environment)\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    q_agent.Q[((0, 1), (0, 1))] >= -0.5\u001b[0m\n",
      "\u001b[0;34m    True\u001b[0m\n",
      "\u001b[0;34m    q_agent.Q[((1, 0), (0, -1))] <= 0.5\u001b[0m\n",
      "\u001b[0;34m    True\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRplus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminals\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_act\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmdp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactlist\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNe\u001b[0m  \u001b[0;31m# iteration limit in exploration function\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRplus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRplus\u001b[0m  \u001b[0;31m# large value to assign before iteration limit\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNsa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# udacity video\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Exploration function. Returns fixed Rplus until\u001b[0m\n",
      "\u001b[0;34m        agent has visited state, action a Ne number of times.\u001b[0m\n",
      "\u001b[0;34m        Same as ADP agent in book.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRplus\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mactions_in_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Return actions possible in given state.\u001b[0m\n",
      "\u001b[0;34m        Useful for max and argmax.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_act\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNsa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNsa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mactions_in_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions_in_state\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mterminals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mNsa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNsa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                                                           \u001b[0;32mfor\u001b[0m \u001b[0ma1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactions_in_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mterminals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions_in_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNsa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mupdate_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercept\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"To be overridden in most cases. The default case\u001b[0m\n",
      "\u001b[0;34m        assumes the percept to be of type (state, reward).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mpercept\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%psource QLearningAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Agent Program can be obtained by creating the instance of the class by passing the appropriate parameters. Because of the __ call __ method the object that is created behaves like a callable and returns an appropriate action as most Agent Programs do. To instantiate the object we need a mdp similar to the PassiveTDAgent.\n",
    "\n",
    " Let us use the same GridMDP object we used above. **Figure 17.1 (sequential_decision_environment)** is similar to **Figure 21.1** but has some discounting as **gamma = 0.9**. The class also implements an exploration function **f** which returns fixed **Rplus** until agent has visited state, action **Ne** number of times. This is the same as the one defined on page **842** of the book. The method **actions_in_state** returns actions possible in given state. It is useful when applying max and argmax operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create our object now. We also use the **same alpha** as given in the footnote of the book on **page 837**. We use **Rplus = 2** and **Ne = 5** as defined on page 843. **Fig 21.7**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_agent = QLearningAgent(sequential_decision_environment, Ne=5, Rplus=2, \n",
    "                         alpha=lambda n: 60./(59+n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to try out the q_agent we make use of the **run_single_trial** function in rl.py (which was also used above). Let us use **200** iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    run_single_trial(q_agent,sequential_decision_environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us see the Q Values. The keys are state-action pairs. Where different actions correspond according to:\n",
    "\n",
    "north = (0, 1)\n",
    "south = (0,-1)\n",
    "west = (-1, 0)\n",
    "east = (1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(float,\n",
       "            {((0, 0), (1, 0)): -0.667401172925876,\n",
       "             ((0, 0), (0, 1)): -0.8341176470588235,\n",
       "             ((0, 0), (-1, 0)): -0.7767134674785574,\n",
       "             ((0, 0), (0, -1)): -0.30841458638936003,\n",
       "             ((1, 0), (1, 0)): -0.2271968693628458,\n",
       "             ((1, 0), (0, 1)): -0.22782697496043106,\n",
       "             ((1, 0), (-1, 0)): -0.26230401371502354,\n",
       "             ((1, 0), (0, -1)): -0.3028849034884784,\n",
       "             ((2, 0), (1, 0)): -0.22816279470758058,\n",
       "             ((2, 0), (0, 1)): -0.23009801353244097,\n",
       "             ((2, 0), (-1, 0)): -0.23332201144278913,\n",
       "             ((2, 0), (0, -1)): -0.22832647294232244,\n",
       "             ((3, 0), (1, 0)): -0.21995571985029946,\n",
       "             ((3, 0), (0, 1)): -0.23032116219212487,\n",
       "             ((3, 0), (-1, 0)): -0.22850564641526055,\n",
       "             ((3, 0), (0, -1)): -0.2273605237669046,\n",
       "             ((4, 0), (1, 0)): -0.2287377576721529,\n",
       "             ((4, 0), (0, 1)): -0.2216801312159856,\n",
       "             ((4, 0), (-1, 0)): -0.238659976532674,\n",
       "             ((4, 0), (0, -1)): -0.24256000577964865,\n",
       "             ((5, 0), (1, 0)): -0.8263141873397084,\n",
       "             ((5, 0), (0, 1)): -0.8876363636363637,\n",
       "             ((5, 0), (-1, 0)): -0.9770718522957695,\n",
       "             ((5, 0), (0, -1)): -0.2591750285130355,\n",
       "             ((6, 0), (1, 0)): -0.2541026800648188,\n",
       "             ((6, 0), (0, 1)): -0.2383909217528491,\n",
       "             ((6, 0), (-1, 0)): -0.2605325883111842,\n",
       "             ((6, 0), (0, -1)): -0.2454643206051552,\n",
       "             ((7, 0), (1, 0)): -0.2523620481865066,\n",
       "             ((7, 0), (0, 1)): -0.2543440196729958,\n",
       "             ((7, 0), (-1, 0)): -0.25517299651556646,\n",
       "             ((7, 0), (0, -1)): -0.25730109275630664,\n",
       "             ((7, 1), (1, 0)): -0.13594879921985684,\n",
       "             ((7, 1), (0, 1)): -0.2613341239943005,\n",
       "             ((7, 1), (-1, 0)): -0.2565917695308325,\n",
       "             ((7, 1), (0, -1)): -0.24408723235192867,\n",
       "             ((7, 2), (1, 0)): -0.8244998457858772,\n",
       "             ((7, 2), (0, 1)): -0.88375,\n",
       "             ((7, 2), (-1, 0)): -0.8707692307692309,\n",
       "             ((7, 2), (0, -1)): -0.04716077702748215,\n",
       "             ((7, 3), (1, 0)): -1.8307692307692307,\n",
       "             ((7, 3), (0, 1)): -1.8307692307692307,\n",
       "             ((7, 3), (-1, 0)): -1.5356696907947214,\n",
       "             ((7, 3), (0, -1)): -1.1676777426636622,\n",
       "             ((7, 4), (1, 0)): -10.0,\n",
       "             ((7, 4), (0, 1)): -10.0,\n",
       "             ((7, 4), (-1, 0)): -9.983329386905647,\n",
       "             ((7, 4), (0, -1)): -10.0,\n",
       "             ((7, 5), (1, 0)): -1.0,\n",
       "             ((7, 5), (0, 1)): 7.012782258064518,\n",
       "             ((7, 5), (-1, 0)): 0.0,\n",
       "             ((7, 5), (0, -1)): 0.0,\n",
       "             ((6, 5), (1, 0)): -0.04,\n",
       "             ((6, 5), (0, 1)): 0.0,\n",
       "             ((6, 5), (-1, 0)): 0.0,\n",
       "             ((6, 5), (0, -1)): 0.0,\n",
       "             ((7, 6), (1, 0)): -0.04,\n",
       "             ((7, 6), (0, 1)): 8.959135944700462,\n",
       "             ((7, 6), (-1, 0)): 0.0,\n",
       "             ((7, 6), (0, -1)): 0.0,\n",
       "             ((7, 7), None): 10,\n",
       "             ((2, 1), (1, 0)): -0.2338647517991995,\n",
       "             ((2, 1), (0, 1)): -0.23840573239641888,\n",
       "             ((2, 1), (-1, 0)): -0.23911052648603995,\n",
       "             ((2, 1), (0, -1)): -0.23208654985933574,\n",
       "             ((3, 1), (1, 0)): -0.24555449366567142,\n",
       "             ((3, 1), (0, 1)): -0.23986194132390445,\n",
       "             ((3, 1), (-1, 0)): -0.24183050885477303,\n",
       "             ((3, 1), (0, -1)): -0.2269138502894415,\n",
       "             ((4, 1), (1, 0)): -0.8264066193656199,\n",
       "             ((4, 1), (0, 1)): -0.8988076923076923,\n",
       "             ((4, 1), (-1, 0)): -0.2404378189841548,\n",
       "             ((4, 1), (0, -1)): -0.8917543349918744,\n",
       "             ((5, 1), (1, 0)): -1.5357142857142865,\n",
       "             ((5, 1), (0, 1)): -10.0,\n",
       "             ((5, 1), (-1, 0)): -1.641948079517289,\n",
       "             ((5, 1), (0, -1)): -1.2162956617608365,\n",
       "             ((6, 1), (1, 0)): -0.24069113675758294,\n",
       "             ((6, 1), (0, 1)): -0.9493132788645999,\n",
       "             ((6, 1), (-1, 0)): -1.0246146958510542,\n",
       "             ((6, 1), (0, -1)): -0.9515808809510035,\n",
       "             ((6, 2), (1, 0)): -1.0,\n",
       "             ((6, 2), (0, 1)): -1.596235835159023,\n",
       "             ((6, 2), (-1, 0)): -10.0,\n",
       "             ((6, 2), (0, -1)): 0.0,\n",
       "             ((0, 1), (1, 0)): -1.2589298724653433,\n",
       "             ((0, 1), (0, 1)): -9.464271549444739,\n",
       "             ((0, 1), (-1, 0)): -1.8307692307692307,\n",
       "             ((0, 1), (0, -1)): -1.194651611351931,\n",
       "             ((1, 1), (1, 0)): -0.8249845789867114,\n",
       "             ((1, 1), (0, 1)): -0.8341176470588235,\n",
       "             ((1, 1), (-1, 0)): -0.9083934861259472,\n",
       "             ((1, 1), (0, -1)): -0.22871961743136474,\n",
       "             ((2, 2), (1, 0)): -0.24448895105377919,\n",
       "             ((2, 2), (0, 1)): -0.8061408361373577,\n",
       "             ((2, 2), (-1, 0)): -0.8733653846153847,\n",
       "             ((2, 2), (0, -1)): -0.24057933371723877,\n",
       "             ((3, 2), (1, 0)): -0.8731716417910448,\n",
       "             ((3, 2), (0, 1)): -0.8888164335664335,\n",
       "             ((3, 2), (-1, 0)): -0.24893560376413626,\n",
       "             ((3, 2), (0, -1)): -0.8967564325463833,\n",
       "             ((4, 2), (1, 0)): -9.999985835159023,\n",
       "             ((4, 2), (0, 1)): -1.036,\n",
       "             ((4, 2), (-1, 0)): -1.1421112313309936,\n",
       "             ((4, 2), (0, -1)): -1.6239721114888868,\n",
       "             ((5, 2), None): -10,\n",
       "             ((0, 2), None): -10,\n",
       "             ((1, 2), (1, 0)): -1.203970252491438,\n",
       "             ((1, 2), (0, 1)): -1.5357142857142865,\n",
       "             ((1, 2), (-1, 0)): -9.46642857142857,\n",
       "             ((1, 2), (0, -1)): -1.182769084867112,\n",
       "             ((6, 3), (1, 0)): -0.04,\n",
       "             ((6, 3), (0, 1)): -0.8838349890458563,\n",
       "             ((6, 3), (-1, 0)): -0.04,\n",
       "             ((6, 3), (0, -1)): 0.0,\n",
       "             ((3, 3), (1, 0)): -0.10660964209369037,\n",
       "             ((3, 3), (0, 1)): -0.07597635125823435,\n",
       "             ((3, 3), (-1, 0)): -0.10359885883347422,\n",
       "             ((3, 3), (0, -1)): -0.11228894505967613,\n",
       "             ((4, 3), (1, 0)): -0.8459701492537314,\n",
       "             ((4, 3), (0, 1)): -0.04,\n",
       "             ((4, 3), (-1, 0)): -0.04,\n",
       "             ((4, 3), (0, -1)): -0.04,\n",
       "             ((5, 3), (1, 0)): -1.0,\n",
       "             ((5, 3), (0, 1)): -1.0,\n",
       "             ((5, 3), (-1, 0)): -1.03375,\n",
       "             ((5, 3), (0, -1)): -10.0,\n",
       "             ((1, 3), (1, 0)): -0.04,\n",
       "             ((1, 3), (0, 1)): -0.04,\n",
       "             ((1, 3), (-1, 0)): -0.04008498904585631,\n",
       "             ((1, 3), (0, -1)): -1.074142504984514,\n",
       "             ((2, 3), (1, 0)): -0.07565807327001359,\n",
       "             ((2, 3), (0, 1)): -0.8462857739275651,\n",
       "             ((2, 3), (-1, 0)): -0.07583445880764038,\n",
       "             ((2, 3), (0, -1)): -0.10178846153846154,\n",
       "             ((3, 4), (1, 0)): -0.04,\n",
       "             ((3, 4), (0, 1)): -0.9399985835159025,\n",
       "             ((3, 4), (-1, 0)): -0.04267715494447383,\n",
       "             ((3, 4), (0, -1)): -0.10829993063336772,\n",
       "             ((3, 5), None): -1,\n",
       "             ((2, 4), (1, 0)): -1.0,\n",
       "             ((2, 4), (0, 1)): -1.0,\n",
       "             ((2, 4), (-1, 0)): -1.0595227007204517,\n",
       "             ((2, 4), (0, -1)): -1.067090519639677,\n",
       "             ((1, 4), (1, 0)): -0.04,\n",
       "             ((1, 4), (0, 1)): -0.04,\n",
       "             ((1, 4), (-1, 0)): -0.04,\n",
       "             ((1, 4), (0, -1)): 0.0,\n",
       "             ((2, 5), (1, 0)): -10.9,\n",
       "             ((2, 5), (0, 1)): -10.0,\n",
       "             ((2, 5), (-1, 0)): 0.0,\n",
       "             ((2, 5), (0, -1)): 0.0,\n",
       "             ((0, 4), (1, 0)): -0.04,\n",
       "             ((0, 4), (0, 1)): 0.0,\n",
       "             ((0, 4), (-1, 0)): 0.0,\n",
       "             ((0, 4), (0, -1)): 0.0,\n",
       "             ((1, 5), (1, 0)): -1.0,\n",
       "             ((1, 5), (0, 1)): 0.0,\n",
       "             ((1, 5), (-1, 0)): 0.0,\n",
       "             ((1, 5), (0, -1)): 0.0,\n",
       "             ((2, 6), (1, 0)): -1.0,\n",
       "             ((2, 6), (0, 1)): 0.0,\n",
       "             ((2, 6), (-1, 0)): 0.0,\n",
       "             ((2, 6), (0, -1)): 0.0,\n",
       "             ((2, 7), (1, 0)): -0.04,\n",
       "             ((2, 7), (0, 1)): 0.0,\n",
       "             ((2, 7), (-1, 0)): 0.0,\n",
       "             ((2, 7), (0, -1)): 0.0,\n",
       "             ((3, 7), (1, 0)): -0.04,\n",
       "             ((3, 7), (0, 1)): 0.0,\n",
       "             ((3, 7), (-1, 0)): 0.0,\n",
       "             ((3, 7), (0, -1)): 0.0,\n",
       "             ((4, 7), (1, 0)): -0.04,\n",
       "             ((4, 7), (0, 1)): 0.0,\n",
       "             ((4, 7), (-1, 0)): 0.0,\n",
       "             ((4, 7), (0, -1)): 0.0,\n",
       "             ((5, 7), (1, 0)): -0.04,\n",
       "             ((5, 7), (0, 1)): 0.0,\n",
       "             ((5, 7), (-1, 0)): 0.0,\n",
       "             ((5, 7), (0, -1)): 0.0,\n",
       "             ((6, 7), (1, 0)): 8.812459016393444,\n",
       "             ((6, 7), (0, 1)): 0.0,\n",
       "             ((6, 7), (-1, 0)): 0.0,\n",
       "             ((6, 7), (0, -1)): 0.0,\n",
       "             ((0, 3), (1, 0)): -1.0047593865679534,\n",
       "             ((0, 3), (0, 1)): 0.0,\n",
       "             ((0, 3), (-1, 0)): 0.0,\n",
       "             ((0, 3), (0, -1)): 0.0})"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_agent.Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Utility **U** of each state is related to **Q** by the following equation.\n",
    "\n",
    "**U (s) = max <sub>a</sub> Q(s, a)**\n",
    "\n",
    "Let us convert the Q Values above into U estimates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U = defaultdict(lambda: -1000.) # Very Large Negative Value for Comparison see below.\n",
    "for state_action, value in q_agent.Q.items():\n",
    "    state, action = state_action\n",
    "    if U[state] < value:\n",
    "                U[state] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {(0, 0): -0.30841458638936003,\n",
       "             (1, 0): -0.2271968693628458,\n",
       "             (2, 0): -0.22816279470758058,\n",
       "             (3, 0): -0.21995571985029946,\n",
       "             (4, 0): -0.2216801312159856,\n",
       "             (5, 0): -0.2591750285130355,\n",
       "             (6, 0): -0.2383909217528491,\n",
       "             (7, 0): -0.2523620481865066,\n",
       "             (7, 1): -0.13594879921985684,\n",
       "             (7, 2): -0.04716077702748215,\n",
       "             (7, 3): -1.1676777426636622,\n",
       "             (7, 4): -9.983329386905647,\n",
       "             (7, 5): 7.012782258064518,\n",
       "             (6, 5): 0.0,\n",
       "             (7, 6): 8.959135944700462,\n",
       "             (7, 7): 10,\n",
       "             (2, 1): -0.23208654985933574,\n",
       "             (3, 1): -0.2269138502894415,\n",
       "             (4, 1): -0.2404378189841548,\n",
       "             (5, 1): -1.2162956617608365,\n",
       "             (6, 1): -0.24069113675758294,\n",
       "             (6, 2): 0.0,\n",
       "             (0, 1): -1.194651611351931,\n",
       "             (1, 1): -0.22871961743136474,\n",
       "             (2, 2): -0.24057933371723877,\n",
       "             (3, 2): -0.24893560376413626,\n",
       "             (4, 2): -1.036,\n",
       "             (5, 2): -10,\n",
       "             (0, 2): -10,\n",
       "             (1, 2): -1.182769084867112,\n",
       "             (6, 3): 0.0,\n",
       "             (3, 3): -0.07597635125823435,\n",
       "             (4, 3): -0.04,\n",
       "             (5, 3): -1.0,\n",
       "             (1, 3): -0.04,\n",
       "             (2, 3): -0.07565807327001359,\n",
       "             (3, 4): -0.04,\n",
       "             (3, 5): -1,\n",
       "             (2, 4): -1.0,\n",
       "             (1, 4): 0.0,\n",
       "             (2, 5): 0.0,\n",
       "             (0, 4): 0.0,\n",
       "             (1, 5): 0.0,\n",
       "             (2, 6): 0.0,\n",
       "             (2, 7): 0.0,\n",
       "             (3, 7): 0.0,\n",
       "             (4, 7): 0.0,\n",
       "             (5, 7): 0.0,\n",
       "             (6, 7): 8.812459016393444,\n",
       "             (0, 3): 0.0})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us finally compare these estimates to value_iteration results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(4, 0): 0.20613401404876092, (3, 4): 0.5027823798502125, (4, 3): 0.4610575123374177, (3, 1): 0.3890609707415478, (3, 7): 5.4768043075879635, (4, 6): 5.7140678387889325, (5, 1): -0.9808577365113227, (5, 7): 7.419331860809735, (0, 2): -10, (0, 5): 2.3882093486949305, (2, 2): 0.6277953174566747, (1, 0): 0.26147733558027636, (1, 6): 3.3139422080663548, (2, 5): -7.807981569055003, (1, 3): 1.266195840691091, (7, 4): -6.615198954253358, (6, 2): -1.3214139727971803, (7, 1): -0.13331442187882087, (7, 7): 10, (6, 5): 6.466073568420779, (4, 2): -0.5720032829935289, (3, 0): 0.2901870316096985, (4, 5): 5.047704330354068, (3, 3): 0.7097815079375986, (5, 0): 0.022130415536142527, (5, 6): 6.561871328704624, (3, 6): 4.689724230968557, (5, 3): -1.7231194166079322, (0, 1): -1.0215163320369014, (0, 7): 3.3426188835734703, (2, 4): -0.15011867142821966, (1, 2): -0.4081752711255061, (0, 4): 2.006145384148317, (2, 1): 0.4728825757574549, (2, 7): 4.588415522502781, (1, 5): 1.1638470983220925, (6, 1): -0.17153477862006347, (7, 0): -0.09149206028607111, (7, 3): -1.4351416381525968, (6, 7): 8.612532741324225, (7, 6): 8.612532741324225, (3, 2): 0.5109083996056243, (4, 1): 0.20719548355751605, (4, 7): 6.3914121376637505, (3, 5): -1, (5, 2): -10, (5, 5): 5.721034493237777, (0, 0): 0.06189755781170282, (1, 1): 0.2872725159659054, (0, 3): 0.6136069248901318, (2, 0): 0.3501251023649489, (1, 4): 1.6231285405746498, (0, 6): 2.9285059284573984, (2, 3): 0.9146518939573437, (1, 7): 3.9141911812390022, (2, 6): 3.023989155714729, (7, 2): -0.2801342719110172, (6, 0): -0.04341262432552723, (6, 6): 7.526719940056038, (7, 5): 6.354912302100343, (6, 3): -1.1584008053380839}\n"
     ]
    }
   ],
   "source": [
    "print(value_iteration(sequential_decision_environment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e627b864e5c744f6bcd67da2c168f83afae70864f34fe154f324beebdf4949f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
